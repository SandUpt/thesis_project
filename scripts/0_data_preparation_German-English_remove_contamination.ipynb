{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "087387e7-4c65-490a-8a9d-6fc2224562c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6fff195-ef31-46fc-8cd7-83339265286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../data\")\n",
    "raw_dir = base_dir / \"raw\" / \"wmt_de_en\"\n",
    "processed_dir = base_dir / \"processed\" / \"de_en\"\n",
    "eval_dir = base_dir / \"evaluation_sets\" / \"de_en\"\n",
    "error_dir = base_dir / \"error_dataset\" / \"de_en\"\n",
    "reserve_dir = base_dir / \"reserve\" / \"de_en\"\n",
    "\n",
    "for dir_path in [processed_dir, eval_dir, error_dir, reserve_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beafbd84-113a-46eb-bddd-8184098203eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 1,838,568\n",
      "Valid pairs: 1,817,761\n"
     ]
    }
   ],
   "source": [
    "train_file = raw_dir / \"train\" / \"europarl-v9.de-en.tsv\"\n",
    "\n",
    "source_texts = []\n",
    "target_texts = []\n",
    "problematic_lines = 0\n",
    "total_lines = 0\n",
    "\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        total_lines += 1\n",
    "        if not line.strip():\n",
    "            continue\n",
    "            \n",
    "        fields = [f.strip() for f in line.strip().split('\\t') if f.strip()]\n",
    "        \n",
    "        if len(fields) == 2:\n",
    "            source_texts.append(fields[0])\n",
    "            target_texts.append(fields[1])\n",
    "        else:\n",
    "            problematic_lines += 1\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'source_de': source_texts,\n",
    "    'target_en': target_texts\n",
    "})\n",
    "\n",
    "print(f\"Total lines: {total_lines:,}\")\n",
    "print(f\"Valid pairs: {len(train_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23a0ed3d-decc-4709-9f32-3c58aa09dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering and deduplication: 1,281,027\n"
     ]
    }
   ],
   "source": [
    "train_df['source_words'] = train_df['source_de'].str.split().str.len()\n",
    "train_df['target_words'] = train_df['target_en'].str.split().str.len()\n",
    "\n",
    "train_filtered = train_df[\n",
    "    (train_df['source_words'] >= 10) & \n",
    "    (train_df['source_words'] <= 40) &\n",
    "    (train_df['target_words'] >= 10) & \n",
    "    (train_df['target_words'] <= 40)\n",
    "].copy()\n",
    "\n",
    "train_filtered = train_filtered.drop(['source_words', 'target_words'], axis=1)\n",
    "train_clean = train_filtered.drop_duplicates(subset=['source_de', 'target_en'])\n",
    "\n",
    "print(f\"After filtering and deduplication: {len(train_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "613df22a-33e6-4512-abdd-a15ea8a24171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sgm_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    soup = BeautifulSoup(content, 'xml')\n",
    "    return [seg.text.strip() for seg in soup.find_all('seg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4de084eb-8dbb-478c-9da7-e2cc53fcea5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test pairs: 30,697\n"
     ]
    }
   ],
   "source": [
    "test_files = [\n",
    "    (\"newstest2019-deen-src.de.sgm\", \"newstest2019-deen-ref.en.sgm\"),\n",
    "    (\"newstest2018-deen-src.de.sgm\", \"newstest2018-deen-ref.en.sgm\"),\n",
    "    (\"newstest2017-deen-src.de.sgm\", \"newstest2017-deen-ref.en.sgm\"),\n",
    "    (\"newstest2016-deen-src.de.sgm\", \"newstest2016-deen-ref.en.sgm\"),\n",
    "    (\"newstest2015-deen-src.de.sgm\", \"newstest2015-deen-ref.en.sgm\"),\n",
    "    (\"newstest2014-deen-src.de.sgm\", \"newstest2014-deen-ref.en.sgm\"),\n",
    "    (\"newstest2013-src.de.sgm\", \"newstest2013-src.en.sgm\"),\n",
    "    (\"newstest2012-src.de.sgm\", \"newstest2012-src.en.sgm\"),\n",
    "    (\"newstest2011-src.de.sgm\", \"newstest2011-src.en.sgm\"),\n",
    "    (\"newstest2010-src.de.sgm\", \"newstest2010-src.en.sgm\"),\n",
    "    (\"newstest2009-src.de.sgm\", \"newstest2009-src.en.sgm\"),\n",
    "    (\"test2008-src.de.sgm\", \"test2008-src.en.sgm\")\n",
    "]\n",
    "\n",
    "all_test_data = []\n",
    "test_dir = raw_dir / \"test\"\n",
    "\n",
    "for de_file, en_file in test_files:\n",
    "    de_path = test_dir / de_file\n",
    "    en_path = test_dir / en_file\n",
    "    \n",
    "    if de_path.exists() and en_path.exists():\n",
    "        de_segments = read_sgm_file(de_path)\n",
    "        en_segments = read_sgm_file(en_path)\n",
    "        \n",
    "        for de, en in zip(de_segments, en_segments):\n",
    "            all_test_data.append({'source_de': de, 'target_en': en})\n",
    "\n",
    "test_df = pd.DataFrame(all_test_data)\n",
    "print(f\"Total test pairs: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "352eb1e0-ce24-409a-984b-f604e1f08918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering and deduplication: 22,414\n"
     ]
    }
   ],
   "source": [
    "test_df['source_words'] = test_df['source_de'].str.split().str.len()\n",
    "test_df['target_words'] = test_df['target_en'].str.split().str.len()\n",
    "\n",
    "test_filtered = test_df[\n",
    "    (test_df['source_words'] >= 10) & \n",
    "    (test_df['source_words'] <= 40) &\n",
    "    (test_df['target_words'] >= 10) & \n",
    "    (test_df['target_words'] <= 40)\n",
    "].copy()\n",
    "\n",
    "test_filtered = test_filtered.drop(['source_words', 'target_words'], axis=1)\n",
    "test_clean = test_filtered.drop_duplicates(subset=['source_de', 'target_en'])\n",
    "\n",
    "print(f\"After filtering and deduplication: {len(test_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c3b9f05-936f-4e4b-a3f8-99cf19e0bccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading contamination sources for exclusion...\n",
      "Error dataset size: 736\n",
      "Loaded isolated_clean_736.tsv: 736 samples\n",
      "Loaded self_correction_isolated_training.tsv: 1472 samples\n",
      "Loaded dev_mini200.tsv: 199 samples\n",
      "Loaded mix2k_dev.tsv: 1999 samples\n",
      "Total contamination samples before dedup: 5142\n",
      "Total unique contamination samples: 3670\n",
      "Final exclusion set size: 3670 unique German sources\n",
      "Test data before contamination removal: 22,414\n",
      "Test data after contamination removal: 19,479\n",
      "Removed: 2935 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2124280/3178665904.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_clean['source_normalized'] = test_clean['source_de'].str.strip().str.lower()\n"
     ]
    }
   ],
   "source": [
    "error_file = error_dir / \"full_analysis_german_low_comet_score_translations_0-5_0-75_final_balanced.xlsx\"\n",
    "error_df = pd.read_excel(error_file)\n",
    "print(f\"Error dataset size: {len(error_df)}\")\n",
    "\n",
    "isolated_clean_file = Path(\"../data/isolated_clean/de_en/isolated_clean_736.tsv\")\n",
    "self_correction_file = Path(\"../data/isolated_clean/de_en/self_correction_isolated_training.tsv\")\n",
    "dev_mini_file = Path(\"../data/processed/de_en/dev_mini200.tsv\")\n",
    "mix2k_dev_file = Path(\"../data/processed/de_en/mix2k_dev.tsv\")\n",
    "\n",
    "def load_contamination_source(file_path, name):\n",
    "    \"\"\"Load contamination source with proper error handling.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"Warning: {name} not found at {file_path}\")\n",
    "        return pd.DataFrame(columns=['source_de', 'target_en'])\n",
    "    \n",
    "    try:\n",
    "        if file_path.suffix == '.xlsx':\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "        \n",
    "        df.columns = [col.strip().lower() for col in df.columns]\n",
    "        \n",
    "        if 'src' in df.columns and 'ref' in df.columns:\n",
    "            df = df[['src', 'ref']].rename(columns={'src': 'source_de', 'ref': 'target_en'})\n",
    "        elif 'source_de' in df.columns and 'target_en' in df.columns:\n",
    "            df = df[['source_de', 'target_en']].copy()\n",
    "        else:\n",
    "            df = df.iloc[:, :2].copy()\n",
    "            df.columns = ['source_de', 'target_en']\n",
    "        \n",
    "        df = df.dropna()\n",
    "        df = df[(df['source_de'].str.strip() != '') & (df['target_en'].str.strip() != '')]\n",
    "        \n",
    "        print(f\"Loaded {name}: {len(df)} samples\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {name}: {e}\")\n",
    "        return pd.DataFrame(columns=['source_de', 'target_en'])\n",
    "\n",
    "isolated_clean_df = load_contamination_source(isolated_clean_file, \"isolated_clean_736.tsv\")\n",
    "self_correction_df = load_contamination_source(self_correction_file, \"self_correction_isolated_training.tsv\")\n",
    "dev_mini_df = load_contamination_source(dev_mini_file, \"dev_mini200.tsv\")\n",
    "mix2k_dev_df = load_contamination_source(mix2k_dev_file, \"mix2k_dev.tsv\")\n",
    "\n",
    "all_contamination_dfs = [error_df, isolated_clean_df, self_correction_df, dev_mini_df, mix2k_dev_df]\n",
    "all_contamination = pd.concat([df for df in all_contamination_dfs if not df.empty], ignore_index=True)\n",
    "\n",
    "print(f\"Total contamination samples before dedup: {len(all_contamination)}\")\n",
    "\n",
    "all_contamination_clean = all_contamination.drop_duplicates(subset=['source_de', 'target_en'])\n",
    "print(f\"Total unique contamination samples: {len(all_contamination_clean)}\")\n",
    "\n",
    "all_contamination_normalized = set(all_contamination_clean['source_de'].str.strip().str.lower())\n",
    "\n",
    "print(f\"Final exclusion set size: {len(all_contamination_normalized)} unique German sources\")\n",
    "\n",
    "test_clean['source_normalized'] = test_clean['source_de'].str.strip().str.lower()\n",
    "test_without_contamination = test_clean[~test_clean['source_normalized'].isin(all_contamination_normalized)].copy()\n",
    "test_without_contamination = test_without_contamination.drop('source_normalized', axis=1)\n",
    "\n",
    "print(f\"Test data before contamination removal: {len(test_clean):,}\")\n",
    "print(f\"Test data after contamination removal: {len(test_without_contamination):,}\")\n",
    "print(f\"Removed: {len(test_clean) - len(test_without_contamination)} sentences\")\n",
    "\n",
    "test_without_error = test_without_contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "178680b2-c5ab-444a-9ce5-7137476373f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying contamination exclusion to training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2124280/3275877677.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_clean['source_normalized'] = train_clean['source_de'].str.strip().str.lower()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data before contamination removal: 1,281,027\n",
      "Training data after contamination removal: 1,280,291\n",
      "Removed: 736 sentences\n"
     ]
    }
   ],
   "source": [
    "# Also exclude contamination from training data\n",
    "print(\"Applying contamination exclusion to training data...\")\n",
    "\n",
    "train_clean['source_normalized'] = train_clean['source_de'].str.strip().str.lower()\n",
    "train_without_contamination = train_clean[~train_clean['source_normalized'].isin(all_contamination_normalized)].copy()\n",
    "train_without_contamination = train_without_contamination.drop('source_normalized', axis=1)\n",
    "\n",
    "print(f\"Training data before contamination removal: {len(train_clean):,}\")\n",
    "print(f\"Training data after contamination removal: {len(train_without_contamination):,}\")\n",
    "print(f\"Removed: {len(train_clean) - len(train_without_contamination)} sentences\")\n",
    "\n",
    "# Update the variable for rest of your notebook\n",
    "train_clean = train_without_contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fe88487-0150-446a-a485-a1c7e89fe68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test evaluation pool: 5,000\n",
      "Test error augmentation pool: 1,000\n",
      "Test reserve pool: 13,479\n"
     ]
    }
   ],
   "source": [
    "test_shuffled = test_without_error.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_eval_pool = test_shuffled.iloc[:5000].copy()\n",
    "test_error_augment_pool = test_shuffled.iloc[5000:6000].copy()  \n",
    "test_reserve_pool = test_shuffled.iloc[6000:].copy()\n",
    "\n",
    "print(f\"Test evaluation pool: {len(test_eval_pool):,}\")\n",
    "print(f\"Test error augmentation pool: {len(test_error_augment_pool):,}\")\n",
    "print(f\"Test reserve pool: {len(test_reserve_pool):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2289e6a-51d8-4d8a-b829-83cd0fed7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pool: 1,152,261\n",
      "Training reserve pool: 128,030\n"
     ]
    }
   ],
   "source": [
    "train_shuffled = train_clean.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "split_point = int(len(train_shuffled) * 0.9)\n",
    "\n",
    "train_pool = train_shuffled.iloc[:split_point].copy()\n",
    "train_reserve_pool = train_shuffled.iloc[split_point:].copy()\n",
    "\n",
    "print(f\"Training pool: {len(train_pool):,}\")\n",
    "print(f\"Training reserve pool: {len(train_reserve_pool):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5bfd65c-1efc-4801-953e-946a7e59b853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train_1000.tsv\n",
      "Saved train_5000.tsv\n",
      "Saved train_10000.tsv\n",
      "Saved train_15000.tsv\n",
      "Saved train_20000.tsv\n",
      "Saved train_30000.tsv\n",
      "Saved train_50000.tsv\n",
      "Saved train_100000.tsv\n",
      "Saved train_200000.tsv\n",
      "Saved train_400000.tsv\n",
      "Saved train_600000.tsv\n",
      "Saved train_800000.tsv\n",
      "Saved train_1000000.tsv\n",
      "Saved train_full.tsv: 1,152,261 samples\n"
     ]
    }
   ],
   "source": [
    "train_sizes = [1000, 5000, 10000, 15000, 20000, 30000, 50000, 100000, 200000, 400000, 600000, 800000, 1000000]\n",
    "\n",
    "for size in train_sizes:\n",
    "    if size <= len(train_pool):\n",
    "        train_subset = train_pool.sample(n=size, random_state=42)\n",
    "        output_path = processed_dir / f\"train_{size}.tsv\"\n",
    "        train_subset.to_csv(output_path, sep='\\t', index=False)\n",
    "        print(f\"Saved train_{size}.tsv\")\n",
    "\n",
    "output_path = processed_dir / \"train_full.tsv\"\n",
    "train_pool.to_csv(output_path, sep='\\t', index=False)\n",
    "print(f\"Saved train_full.tsv: {len(train_pool):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b7a7274-a6ac-4a78-8606-30371dda3745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test_500_clean.tsv\n",
      "Saved test_1000_clean.tsv\n",
      "Saved test_2000_clean.tsv\n",
      "Saved test_5000_clean.tsv\n"
     ]
    }
   ],
   "source": [
    "test_sizes = [500, 1000, 2000, 5000]\n",
    "\n",
    "for size in test_sizes:\n",
    "    test_subset = test_eval_pool.sample(n=size, random_state=42)\n",
    "    output_path = eval_dir / f\"test_{size}_clean.tsv\"\n",
    "    test_subset.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"Saved test_{size}_clean.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f3f96b0-ef87-4d32-a36f-086da9122b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reserve: 13,479 samples\n",
      "train reserve: 128,030 samples\n",
      "error augmentation pool: 1,000 samples\n"
     ]
    }
   ],
   "source": [
    "test_reserve_path = reserve_dir / \"test_reserve.tsv\"\n",
    "test_reserve_pool.to_csv(test_reserve_path, sep='\\t', index=False)\n",
    "print(f\"test reserve: {len(test_reserve_pool):,} samples\")\n",
    "\n",
    "train_reserve_path = reserve_dir / \"train_reserve.tsv\"\n",
    "train_reserve_pool.to_csv(train_reserve_path, sep='\\t', index=False)\n",
    "print(f\"train reserve: {len(train_reserve_pool):,} samples\")\n",
    "\n",
    "error_augment_path = reserve_dir / \"test_error_augment_pool.tsv\"\n",
    "test_error_augment_pool.to_csv(error_augment_path, sep='\\t', index=False)\n",
    "print(f\"error augmentation pool: {len(test_error_augment_pool):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33004a0c-8b48-4712-a6d6-0163c85e6283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Contamination Check:\n",
      "Test vs Error dataset: 0 overlaps\n",
      "Train vs Error dataset: 0 overlaps\n",
      "Test vs Isolated clean: 0 overlaps\n",
      "Train vs Isolated clean: 0 overlaps\n",
      "Test vs Self correction: 0 overlaps\n",
      "Train vs Self correction: 0 overlaps\n",
      "Test vs Dev mini: 0 overlaps\n",
      "Train vs Dev mini: 0 overlaps\n",
      "Test vs Mix2k dev: 0 overlaps\n",
      "Train vs Mix2k dev: 0 overlaps\n",
      "Test vs Train: 0 overlaps\n",
      "No contamination\n"
     ]
    }
   ],
   "source": [
    "print(\"Comprehensive Contamination Check:\")\n",
    "\n",
    "train_sample = pd.read_csv(processed_dir / \"train_50000.tsv\", delimiter='\\t')\n",
    "test_sample = pd.read_csv(eval_dir / \"test_2000_clean.tsv\", delimiter='\\t')\n",
    "\n",
    "train_normalized = set(train_sample['source_de'].str.strip().str.lower())\n",
    "test_normalized = set(test_sample['source_de'].str.strip().str.lower())\n",
    "\n",
    "contamination_checks = {\n",
    "    \"Error dataset\": set(error_df['source_de'].str.strip().str.lower()),\n",
    "    \"Isolated clean\": set(isolated_clean_df['source_de'].str.strip().str.lower()) if not isolated_clean_df.empty else set(),\n",
    "    \"Self correction\": set(self_correction_df['source_de'].str.strip().str.lower()) if not self_correction_df.empty else set(),\n",
    "    \"Dev mini\": set(dev_mini_df['source_de'].str.strip().str.lower()) if not dev_mini_df.empty else set(),\n",
    "    \"Mix2k dev\": set(mix2k_dev_df['source_de'].str.strip().str.lower()) if not mix2k_dev_df.empty else set(),\n",
    "}\n",
    "\n",
    "for name, contamination_set in contamination_checks.items():\n",
    "    if contamination_set:  # Only check non-empty sets\n",
    "        test_overlap = len(test_normalized.intersection(contamination_set))\n",
    "        train_overlap = len(train_normalized.intersection(contamination_set))\n",
    "        print(f\"Test vs {name}: {test_overlap} overlaps\")\n",
    "        print(f\"Train vs {name}: {train_overlap} overlaps\")\n",
    "\n",
    "test_train_overlap = len(test_normalized.intersection(train_normalized))\n",
    "print(f\"Test vs Train: {test_train_overlap} overlaps\")\n",
    "\n",
    "total_contamination = sum([\n",
    "    len(test_normalized.intersection(contamination_set)) \n",
    "    for contamination_set in contamination_checks.values() \n",
    "    if contamination_set\n",
    "]) + test_train_overlap\n",
    "\n",
    "if total_contamination == 0:\n",
    "    print(\"No contamination\")\n",
    "else:\n",
    "    print(f\"Total contamination detected: {total_contamination} overlaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a18e70e8-921b-473f-b456-fe0a8d79c46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Dataset Verification:\n",
      "All contamination vs test_eval_pool: 0 overlaps\n",
      "All contamination vs test_error_augment: 0 overlaps\n",
      "All contamination vs test_reserve: 0 overlaps\n",
      "All contamination vs train_pool: 0 overlaps\n",
      "All contamination vs train_reserve: 0 overlaps\n",
      "\n",
      "Reserve Pools Verification:\n",
      "train_reserve vs test_reserve: 0 overlaps\n",
      "train_reserve vs test_error_augment: 0 overlaps\n",
      "test_reserve vs test_error_augment: 0 overlaps\n",
      "train_reserve vs test_eval: 0 overlaps\n",
      "test_reserve vs test_eval: 0 overlaps\n",
      "\n",
      "Individual Contamination Source Checks:\n",
      "Isolated clean vs train_pool: 0 overlaps\n",
      "Self correction vs train_pool: 0 overlaps\n"
     ]
    }
   ],
   "source": [
    "print(\"Comprehensive Dataset Verification:\")\n",
    "\n",
    "print(f\"All contamination vs test_eval_pool: {len(all_contamination_normalized.intersection(set(test_eval_pool['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"All contamination vs test_error_augment: {len(all_contamination_normalized.intersection(set(test_error_augment_pool['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"All contamination vs test_reserve: {len(all_contamination_normalized.intersection(set(test_reserve_pool['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"All contamination vs train_pool: {len(all_contamination_normalized.intersection(set(train_pool.sample(n=50000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"All contamination vs train_reserve: {len(all_contamination_normalized.intersection(set(train_reserve_pool.sample(n=10000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "\n",
    "print(\"\\nReserve Pools Verification:\")\n",
    "train_reserve_norm = set(train_reserve_pool.sample(n=10000, random_state=42)['source_de'].str.strip().str.lower())\n",
    "test_reserve_norm = set(test_reserve_pool.sample(n=5000, random_state=42)['source_de'].str.strip().str.lower())\n",
    "test_error_augment_norm = set(test_error_augment_pool['source_de'].str.strip().str.lower())\n",
    "\n",
    "print(f\"train_reserve vs test_reserve: {len(train_reserve_norm.intersection(test_reserve_norm))} overlaps\")\n",
    "print(f\"train_reserve vs test_error_augment: {len(train_reserve_norm.intersection(test_error_augment_norm))} overlaps\")\n",
    "print(f\"test_reserve vs test_error_augment: {len(test_reserve_norm.intersection(test_error_augment_norm))} overlaps\")\n",
    "print(f\"train_reserve vs test_eval: {len(train_reserve_norm.intersection(set(test_eval_pool.sample(n=2000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"test_reserve vs test_eval: {len(test_reserve_norm.intersection(set(test_eval_pool.sample(n=2000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "\n",
    "print(\"\\nIndividual Contamination Source Checks:\")\n",
    "if not isolated_clean_df.empty:\n",
    "    isolated_norm = set(isolated_clean_df['source_de'].str.strip().str.lower())\n",
    "    print(f\"Isolated clean vs train_pool: {len(isolated_norm.intersection(set(train_pool.sample(n=50000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "\n",
    "if not self_correction_df.empty:\n",
    "    self_corr_norm = set(self_correction_df['source_de'].str.strip().str.lower())\n",
    "    print(f\"Self correction vs train_pool: {len(self_corr_norm.intersection(set(train_pool.sample(n=50000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9985643-5b16-43d8-b64e-e9d05935f8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548edec-f736-4721-b206-2cc2acebc852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4eb8e-e1d9-4d46-a832-4bfdae190553",
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating mixed train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffd1876b-fcd2-4517-a7e1-34cadb3a7157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 15k: 15000\n",
      "Test reserve: 13479\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_15K_PATH = \"/sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/data/processed/de_en/train_15000.tsv\"\n",
    "TEST_RESERVE_PATH = \"/sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/data/reserve/de_en/test_reserve.tsv\"\n",
    "OUTPUT_DIR = Path(\"/sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/data/processed/de_en/\")\n",
    "\n",
    "train_15k = pd.read_csv(TRAIN_15K_PATH, sep='\\t', dtype=str, keep_default_na=False)\n",
    "test_reserve = pd.read_csv(TEST_RESERVE_PATH, sep='\\t', dtype=str, keep_default_na=False)\n",
    "\n",
    "print(f\"Train 15k: {len(train_15k)}\")\n",
    "print(f\"Test reserve: {len(test_reserve)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eea88efd-b1fd-444d-a026-a46887b9fbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: train_europarl_newstest_balanced_26000.tsv\n",
      "Size: 26000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_13k = train_15k.sample(n=13000, random_state=42).reset_index(drop=True)\n",
    "test_reserve_13k = test_reserve.sample(n=13000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "combined = pd.concat([train_13k, test_reserve_13k], ignore_index=True)\n",
    "combined_dedup = combined.drop_duplicates(subset=['source_de', 'target_en'], keep='first')\n",
    "combined_final = combined_dedup.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "filename = f\"train_europarl_newstest_balanced_{len(combined_final)}.tsv\"\n",
    "output_path = OUTPUT_DIR / filename\n",
    "combined_final.to_csv(output_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Created: {filename}\")\n",
    "print(f\"Size: {len(combined_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67be1cf-0d63-4397-930b-4c45aab6f568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination overlap: 0\n"
     ]
    }
   ],
   "source": [
    "# contamination check\n",
    "def load_contamination_sources():\n",
    "    sources = []\n",
    "    paths = [\n",
    "        \"../data/isolated_clean/de_en/isolated_clean_736.tsv\",\n",
    "        \"../data/isolated_clean/de_en/self_correction_isolated_training.tsv\", \n",
    "        \"../data/processed/de_en/dev_mini200.tsv\",\n",
    "        \"../data/processed/de_en/mix2k_dev.tsv\"\n",
    "    ]\n",
    "    \n",
    "    for path in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep='\\t')\n",
    "            sources.extend(df.iloc[:, 0].str.strip().str.lower().tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return set(sources)\n",
    "\n",
    "contamination_sources = load_contamination_sources()\n",
    "combined_sources = set(combined_final['source_de'].str.strip().str.lower())\n",
    "overlap = len(combined_sources & contamination_sources)\n",
    "\n",
    "print(f\"Contamination overlap: {overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778f9a7-b493-4254-a8f0-fdd8ec292f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (User + Packages)",
   "language": "python",
   "name": "user-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
