{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087387e7-4c65-490a-8a9d-6fc2224562c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fff195-ef31-46fc-8cd7-83339265286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../data\")\n",
    "raw_dir = base_dir / \"raw\" / \"wmt_de_en\"\n",
    "processed_dir = base_dir / \"processed\" / \"de_en\"\n",
    "eval_dir = base_dir / \"evaluation_sets\" / \"de_en\"\n",
    "error_dir = base_dir / \"error_dataset\" / \"de_en\"\n",
    "reserve_dir = base_dir / \"reserve\" / \"de_en\"\n",
    "\n",
    "for dir_path in [processed_dir, eval_dir, error_dir, reserve_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beafbd84-113a-46eb-bddd-8184098203eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 1,838,568\n",
      "Valid pairs: 1,817,761\n"
     ]
    }
   ],
   "source": [
    "train_file = raw_dir / \"train\" / \"europarl-v9.de-en.tsv\"\n",
    "\n",
    "source_texts = []\n",
    "target_texts = []\n",
    "problematic_lines = 0\n",
    "total_lines = 0\n",
    "\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        total_lines += 1\n",
    "        if not line.strip():\n",
    "            continue\n",
    "            \n",
    "        fields = [f.strip() for f in line.strip().split('\\t') if f.strip()]\n",
    "        \n",
    "        if len(fields) == 2:\n",
    "            source_texts.append(fields[0])\n",
    "            target_texts.append(fields[1])\n",
    "        else:\n",
    "            problematic_lines += 1\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'source_de': source_texts,\n",
    "    'target_en': target_texts\n",
    "})\n",
    "\n",
    "print(f\"Total lines: {total_lines:,}\")\n",
    "print(f\"Valid pairs: {len(train_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a0ed3d-decc-4709-9f32-3c58aa09dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering and deduplication: 1,281,027\n"
     ]
    }
   ],
   "source": [
    "train_df['source_words'] = train_df['source_de'].str.split().str.len()\n",
    "train_df['target_words'] = train_df['target_en'].str.split().str.len()\n",
    "\n",
    "train_filtered = train_df[\n",
    "    (train_df['source_words'] >= 10) & \n",
    "    (train_df['source_words'] <= 40) &\n",
    "    (train_df['target_words'] >= 10) & \n",
    "    (train_df['target_words'] <= 40)\n",
    "].copy()\n",
    "\n",
    "train_filtered = train_filtered.drop(['source_words', 'target_words'], axis=1)\n",
    "train_clean = train_filtered.drop_duplicates(subset=['source_de', 'target_en'])\n",
    "\n",
    "print(f\"After filtering and deduplication: {len(train_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613df22a-33e6-4512-abdd-a15ea8a24171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sgm_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    soup = BeautifulSoup(content, 'xml')\n",
    "    return [seg.text.strip() for seg in soup.find_all('seg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de084eb-8dbb-478c-9da7-e2cc53fcea5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test pairs: 30,697\n"
     ]
    }
   ],
   "source": [
    "test_files = [\n",
    "    (\"newstest2019-deen-src.de.sgm\", \"newstest2019-deen-ref.en.sgm\"),\n",
    "    (\"newstest2018-deen-src.de.sgm\", \"newstest2018-deen-ref.en.sgm\"),\n",
    "    (\"newstest2017-deen-src.de.sgm\", \"newstest2017-deen-ref.en.sgm\"),\n",
    "    (\"newstest2016-deen-src.de.sgm\", \"newstest2016-deen-ref.en.sgm\"),\n",
    "    (\"newstest2015-deen-src.de.sgm\", \"newstest2015-deen-ref.en.sgm\"),\n",
    "    (\"newstest2014-deen-src.de.sgm\", \"newstest2014-deen-ref.en.sgm\"),\n",
    "    (\"newstest2013-src.de.sgm\", \"newstest2013-src.en.sgm\"),\n",
    "    (\"newstest2012-src.de.sgm\", \"newstest2012-src.en.sgm\"),\n",
    "    (\"newstest2011-src.de.sgm\", \"newstest2011-src.en.sgm\"),\n",
    "    (\"newstest2010-src.de.sgm\", \"newstest2010-src.en.sgm\"),\n",
    "    (\"newstest2009-src.de.sgm\", \"newstest2009-src.en.sgm\"),\n",
    "    (\"test2008-src.de.sgm\", \"test2008-src.en.sgm\")\n",
    "]\n",
    "\n",
    "all_test_data = []\n",
    "test_dir = raw_dir / \"test\"\n",
    "\n",
    "for de_file, en_file in test_files:\n",
    "    de_path = test_dir / de_file\n",
    "    en_path = test_dir / en_file\n",
    "    \n",
    "    if de_path.exists() and en_path.exists():\n",
    "        de_segments = read_sgm_file(de_path)\n",
    "        en_segments = read_sgm_file(en_path)\n",
    "        \n",
    "        for de, en in zip(de_segments, en_segments):\n",
    "            all_test_data.append({'source_de': de, 'target_en': en})\n",
    "\n",
    "test_df = pd.DataFrame(all_test_data)\n",
    "print(f\"Total test pairs: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352eb1e0-ce24-409a-984b-f604e1f08918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering and deduplication: 22,414\n"
     ]
    }
   ],
   "source": [
    "test_df['source_words'] = test_df['source_de'].str.split().str.len()\n",
    "test_df['target_words'] = test_df['target_en'].str.split().str.len()\n",
    "\n",
    "test_filtered = test_df[\n",
    "    (test_df['source_words'] >= 10) & \n",
    "    (test_df['source_words'] <= 40) &\n",
    "    (test_df['target_words'] >= 10) & \n",
    "    (test_df['target_words'] <= 40)\n",
    "].copy()\n",
    "\n",
    "test_filtered = test_filtered.drop(['source_words', 'target_words'], axis=1)\n",
    "test_clean = test_filtered.drop_duplicates(subset=['source_de', 'target_en'])\n",
    "\n",
    "print(f\"After filtering and deduplication: {len(test_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c3b9f05-936f-4e4b-a3f8-99cf19e0bccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error dataset size: 736\n",
      "Test data after removing error dataset: 21,679\n",
      "Removed: 735 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2796266/2424853144.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_clean['source_normalized'] = test_clean['source_de'].str.strip().str.lower()\n"
     ]
    }
   ],
   "source": [
    "error_file = error_dir / \"full_analysis_german_low_comet_score_translations_0-5_0-75_final_balanced.xlsx\"\n",
    "error_df = pd.read_excel(error_file)\n",
    "\n",
    "print(f\"Error dataset size: {len(error_df)}\")\n",
    "\n",
    "error_sources_normalized = set(error_df['source_de'].str.strip().str.lower())\n",
    "\n",
    "test_clean['source_normalized'] = test_clean['source_de'].str.strip().str.lower()\n",
    "test_without_error = test_clean[~test_clean['source_normalized'].isin(error_sources_normalized)].copy()\n",
    "test_without_error = test_without_error.drop('source_normalized', axis=1)\n",
    "\n",
    "print(f\"Test data after removing error dataset: {len(test_without_error):,}\")\n",
    "print(f\"Removed: {len(test_clean) - len(test_without_error)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe88487-0150-446a-a485-a1c7e89fe68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test evaluation pool: 5,000\n",
      "Test error augmentation pool: 1,000\n",
      "Test reserve pool: 15,679\n"
     ]
    }
   ],
   "source": [
    "test_shuffled = test_without_error.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_eval_pool = test_shuffled.iloc[:5000].copy()\n",
    "test_error_augment_pool = test_shuffled.iloc[5000:6000].copy()  \n",
    "test_reserve_pool = test_shuffled.iloc[6000:].copy()\n",
    "\n",
    "print(f\"Test evaluation pool: {len(test_eval_pool):,}\")\n",
    "print(f\"Test error augmentation pool: {len(test_error_augment_pool):,}\")\n",
    "print(f\"Test reserve pool: {len(test_reserve_pool):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2289e6a-51d8-4d8a-b829-83cd0fed7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pool: 1,152,924\n",
      "Training reserve pool: 128,103\n"
     ]
    }
   ],
   "source": [
    "train_shuffled = train_clean.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "split_point = int(len(train_shuffled) * 0.9)\n",
    "\n",
    "train_pool = train_shuffled.iloc[:split_point].copy()\n",
    "train_reserve_pool = train_shuffled.iloc[split_point:].copy()\n",
    "\n",
    "print(f\"Training pool: {len(train_pool):,}\")\n",
    "print(f\"Training reserve pool: {len(train_reserve_pool):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5bfd65c-1efc-4801-953e-946a7e59b853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train_1000.tsv\n",
      "Saved train_5000.tsv\n",
      "Saved train_10000.tsv\n",
      "Saved train_20000.tsv\n",
      "Saved train_50000.tsv\n",
      "Saved train_100000.tsv\n",
      "Saved train_200000.tsv\n",
      "Saved train_400000.tsv\n",
      "Saved train_600000.tsv\n",
      "Saved train_800000.tsv\n",
      "Saved train_1000000.tsv\n",
      "Saved train_full.tsv: 1,152,924 samples\n"
     ]
    }
   ],
   "source": [
    "train_sizes = [1000, 5000, 10000, 20000, 50000, 100000, 200000, 400000, 600000, 800000, 1000000]\n",
    "\n",
    "for size in train_sizes:\n",
    "    if size <= len(train_pool):\n",
    "        train_subset = train_pool.sample(n=size, random_state=42)\n",
    "        output_path = processed_dir / f\"train_{size}.tsv\"\n",
    "        train_subset.to_csv(output_path, sep='\\t', index=False)\n",
    "        print(f\"Saved train_{size}.tsv\")\n",
    "\n",
    "output_path = processed_dir / \"train_full.tsv\"\n",
    "train_pool.to_csv(output_path, sep='\\t', index=False)\n",
    "print(f\"Saved train_full.tsv: {len(train_pool):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b7a7274-a6ac-4a78-8606-30371dda3745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test_500_clean.tsv\n",
      "Saved test_1000_clean.tsv\n",
      "Saved test_2000_clean.tsv\n",
      "Saved test_5000_clean.tsv\n"
     ]
    }
   ],
   "source": [
    "test_sizes = [500, 1000, 2000, 5000]\n",
    "\n",
    "for size in test_sizes:\n",
    "    test_subset = test_eval_pool.sample(n=size, random_state=42)\n",
    "    output_path = eval_dir / f\"test_{size}_clean.tsv\"\n",
    "    test_subset.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"Saved test_{size}_clean.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3f96b0-ef87-4d32-a36f-086da9122b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reserve: 15,679 samples\n",
      "train reserve: 128,103 samples\n",
      "error augmentation pool: 1,000 samples\n"
     ]
    }
   ],
   "source": [
    "test_reserve_path = reserve_dir / \"test_reserve.tsv\"\n",
    "test_reserve_pool.to_csv(test_reserve_path, sep='\\t', index=False)\n",
    "print(f\"test reserve: {len(test_reserve_pool):,} samples\")\n",
    "\n",
    "train_reserve_path = reserve_dir / \"train_reserve.tsv\"\n",
    "train_reserve_pool.to_csv(train_reserve_path, sep='\\t', index=False)\n",
    "print(f\"train reserve: {len(train_reserve_pool):,} samples\")\n",
    "\n",
    "error_augment_path = reserve_dir / \"test_error_augment_pool.tsv\"\n",
    "test_error_augment_pool.to_csv(error_augment_path, sep='\\t', index=False)\n",
    "print(f\"error augmentation pool: {len(test_error_augment_pool):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33004a0c-8b48-4712-a6d6-0163c85e6283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination Check:\n",
      "Test vs Train: 0 overlaps\n",
      "Test vs Error: 0 overlaps\n",
      "Train vs Error: 0 overlaps\n",
      "No contamination in test set\n"
     ]
    }
   ],
   "source": [
    "train_sample = pd.read_csv(processed_dir / \"train_50000.tsv\", delimiter='\\t')\n",
    "test_sample = pd.read_csv(eval_dir / \"test_2000_clean.tsv\", delimiter='\\t')\n",
    "\n",
    "train_normalized = set(train_sample['source_de'].str.strip().str.lower())\n",
    "test_normalized = set(test_sample['source_de'].str.strip().str.lower())\n",
    "error_normalized = set(error_df['source_de'].str.strip().str.lower())\n",
    "\n",
    "test_train_overlap = len(test_normalized.intersection(train_normalized))\n",
    "test_error_overlap = len(test_normalized.intersection(error_normalized))\n",
    "train_error_overlap = len(train_normalized.intersection(error_normalized))\n",
    "\n",
    "print(\"Contamination Check:\")\n",
    "print(f\"Test vs Train: {test_train_overlap} overlaps\")\n",
    "print(f\"Test vs Error: {test_error_overlap} overlaps\")\n",
    "print(f\"Train vs Error: {train_error_overlap} overlaps\")\n",
    "\n",
    "if test_train_overlap == 0 and test_error_overlap == 0:\n",
    "    print(\"No contamination in test set\")\n",
    "else:\n",
    "    print(\"Contamination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a18e70e8-921b-473f-b456-fe0a8d79c46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Dataset Verification:\n",
      "Error vs test_eval_pool: 0 overlaps\n",
      "Error vs test_error_augment: 0 overlaps\n",
      "Error vs test_reserve: 0 overlaps\n",
      "Error vs train_pool: 0 overlaps\n",
      "Error vs train_reserve: 0 overlaps\n",
      "\n",
      "Reserve Pools Verification:\n",
      "train_reserve vs test_reserve: 0 overlaps\n",
      "train_reserve vs test_error_augment: 0 overlaps\n",
      "test_reserve vs test_error_augment: 0 overlaps\n",
      "train_reserve vs test_eval: 0 overlaps\n",
      "test_reserve vs test_eval: 0 overlaps\n"
     ]
    }
   ],
   "source": [
    "print(\"Error Dataset Verification:\")\n",
    "print(f\"Error vs test_eval_pool: {len(error_normalized.intersection(set(test_eval_pool['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"Error vs test_error_augment: {len(error_normalized.intersection(set(test_error_augment_pool['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"Error vs test_reserve: {len(error_normalized.intersection(set(test_reserve_pool['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"Error vs train_pool: {len(error_normalized.intersection(set(train_pool.sample(n=50000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"Error vs train_reserve: {len(error_normalized.intersection(set(train_reserve_pool.sample(n=10000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "\n",
    "print(\"\\nReserve Pools Verification:\")\n",
    "train_reserve_norm = set(train_reserve_pool.sample(n=10000, random_state=42)['source_de'].str.strip().str.lower())\n",
    "test_reserve_norm = set(test_reserve_pool.sample(n=5000, random_state=42)['source_de'].str.strip().str.lower())\n",
    "test_error_augment_norm = set(test_error_augment_pool['source_de'].str.strip().str.lower())\n",
    "\n",
    "print(f\"train_reserve vs test_reserve: {len(train_reserve_norm.intersection(test_reserve_norm))} overlaps\")\n",
    "print(f\"train_reserve vs test_error_augment: {len(train_reserve_norm.intersection(test_error_augment_norm))} overlaps\")\n",
    "print(f\"test_reserve vs test_error_augment: {len(test_reserve_norm.intersection(test_error_augment_norm))} overlaps\")\n",
    "\n",
    "print(f\"train_reserve vs test_eval: {len(train_reserve_norm.intersection(set(test_eval_pool.sample(n=2000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")\n",
    "print(f\"test_reserve vs test_eval: {len(test_reserve_norm.intersection(set(test_eval_pool.sample(n=2000, random_state=42)['source_de'].str.strip().str.lower())))} overlaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9985643-5b16-43d8-b64e-e9d05935f8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (User + Packages)",
   "language": "python",
   "name": "user-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
