{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ee676b-f469-4d45-9b30-fa024f612fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.2\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Initial imports and setup\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import os\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Add HF token\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db2c89c-259c-4f4b-9a7c-b9f90f10d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU Memory Usage:\n",
      "Allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.5 (New cell to add between 1 and 2)\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Current GPU Memory Usage:\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Print initial memory state\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e89bc-1de6-405b-aa39-18f3ac81a5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f2164f-e4ad-45e2-8bdc-018ed84e0d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b22311e1829491ba5d763b735757f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU Memory Usage:\n",
      "Allocated: 6.62 GB\n",
      "Cached: 6.62 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Base Model and Tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Clear CUDA cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=True  # Add 8-bit quantization\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3179f01-5db1-4c7d-96db-25efb13dcb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT validation data...\n",
      "Loaded 20 examples for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Validation Data\n",
    "def load_eval_data(num_examples=20):  # Reduced from 100 to 20\n",
    "    print(\"Loading WMT validation data...\")\n",
    "    eval_data = load_dataset(\"wmt19\", \"de-en\", split=\"validation\")\n",
    "    \n",
    "    # Take subset for testing\n",
    "    eval_data = eval_data.select(range(min(num_examples, len(eval_data))))\n",
    "    return eval_data\n",
    "\n",
    "eval_dataset = load_eval_data()\n",
    "print(f\"Loaded {len(eval_dataset)} examples for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da9149a-b0cf-4fc4-958c-b69cf839e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Translation Function\n",
    "def translate(text, src_lang=\"German\", tgt_lang=\"English\", max_length=128):\n",
    "    prompt = f\"Translate from {src_lang} to {tgt_lang}:\\n{text}\\nTranslation:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    \n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    translation = translation.split(\"Translation:\")[-1].strip()\n",
    "    translation = translation.split('\\n')[0]\n",
    "    \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6214fa4-b227-4026-8798-3d30cd573f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting translation evaluation on 20 examples...\n",
      "Showing first 10 translations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|                                                                               | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 0:\n",
      "Source: München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\n",
      "Translation: Munich 1856: Four Cards That Will Change Your View of the City\n",
      "Reference: Munich 1856: Four maps that will change your view of the city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  10%|███████                                                                | 1/10 [00:19<02:59, 19.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Source: Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\n",
      "Translation: An insane asylum, where today's youth should meet.\n",
      "Reference: A mental asylum, where today young people are said to meet.\n",
      "\n",
      "Example 2:\n",
      "Source: Eine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\n",
      "Translation: A graveyard chapel, where now the S-Bahn tunnel is being dug.\n",
      "Reference: A crypt chapel, where they are now digging tunnels for the S-Bahn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  20%|██████████████▏                                                        | 2/10 [00:39<02:39, 19.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 3:\n",
      "Source: Kleingärtner bewirtschaften den einstigen Grund von Bauern.\n",
      "Translation: Small gardeners cultivate the former land of farmers.\n",
      "Reference: Allotment holders cultivate the soil of former farmers.\n",
      "\n",
      "Example 4:\n",
      "Source: Die älteste offizielle Karte Münchens fördert spannende Geschichten zu Tage.\n",
      "Translation: The oldest official map of Munich brings exciting stories to light.\n",
      "Reference: The oldest official map of Munich brings captivating stories to light.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  30%|█████████████████████▎                                                 | 3/10 [00:54<02:02, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 5:\n",
      "Source: Es nervt, wenn Landkarten nicht aktuell sind.\n",
      "Translation: It bothers me when maps are not up-to-date.\n",
      "Reference: It is annoying when geographical maps are not up-to-date.\n",
      "\n",
      "Example 6:\n",
      "Source: Das kennt jeder, der sich schon mal aufregen musste, weil das Auto-Navi statt einer Umgehungsstraße eine grüne Wiese anzeigte.\n",
      "Translation: Everyone who has ever gotten upset because the car navigation system instead of a detour showed a green meadow knows that.\n",
      "Reference: Anyone who has ever got worked up because the car's sat-nav is showing a green field instead of a bypass knows that.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  40%|████████████████████████████▍                                          | 4/10 [01:14<01:50, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 7:\n",
      "Source: Die historischen Landkarten des digitalen Bayern-Atlases, ein Angebot des Geoportals Bayern der Staatsregierung, sind alles andere als aktuell - doch gerade deshalb sehr aufschlussreich.\n",
      "Translation: The historical maps of the digital Bavarian Atlas, an offer of the Geoportal Bavaria of the Bavarian State Government, are anything but up-to-date - but precisely for this reason very informative.\n",
      "Reference: The historical maps of the digital BayernAtlas, an offering from the State Government's Geoportal Bayern, are anything but up-to-date – and yet it is precisely for this reason that they are so informative.\n",
      "\n",
      "Example 8:\n",
      "Source: Besonders wenn man sie mit aktuellen Online-Karten vergleicht.\n",
      "Translation: Particularly if you compare them with current online maps.\n",
      "Reference: Especially when one compares them with current online maps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  50%|███████████████████████████████████▌                                   | 5/10 [01:34<01:34, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 9:\n",
      "Source: Dann wird deutlich, wie sich Städte und Gemeinden im Verbreitungsgebiet des Münchner Merkur seit dem 19. Jahrhundert verändert haben.\n",
      "Translation: Dann wird deutlich, wie sich Städte und Gemeinden im Verbreitungsgebiet des Münchner Merkur seit dem 19. Jahrhundert verändert haben.\n",
      "Reference: Then it becomes clear how the towns and municipalities in the distribution area of Munich's Merkur newspaper have changed since the 19th century.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████████████████████████████████████████████████████████████████| 10/10 [03:10<00:00, 19.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU score: 22.997519112894437\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264af4b0de4d45ec89211e7110c71a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET score: 0.7877008706331253\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Initial Evaluation\n",
    "def evaluate_translations(num_examples=20, num_to_show=10, batch_size=2):\n",
    "    print(f\"Starting translation evaluation on {num_examples} examples...\")\n",
    "    print(f\"Showing first {num_to_show} translations...\")\n",
    "    \n",
    "    # Take subset of evaluation data\n",
    "    eval_subset = eval_dataset.select(range(num_examples))\n",
    "    \n",
    "    translations = []\n",
    "    references = []\n",
    "    sources = []  # Add a list to store source texts\n",
    "    \n",
    "    # Process in smaller batches\n",
    "    for i in tqdm(range(0, len(eval_subset), batch_size), desc=\"Translating\"):\n",
    "        # Get batch indices\n",
    "        end_idx = min(i + batch_size, len(eval_subset))\n",
    "        batch = eval_subset.select(range(i, end_idx))\n",
    "        \n",
    "        for example in batch:\n",
    "            source_text = example['translation']['de']\n",
    "            reference = example['translation']['en']\n",
    "            \n",
    "            # Clear cache before each translation\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            translation = translate(source_text)\n",
    "            \n",
    "            translations.append(translation)\n",
    "            references.append([reference])\n",
    "            sources.append(source_text)  # Store source text\n",
    "            \n",
    "            if len(translations) <= num_to_show:\n",
    "                print(f\"\\nExample {len(translations)-1}:\")\n",
    "                print(\"Source:\", source_text)\n",
    "                print(\"Translation:\", translation)\n",
    "                print(\"Reference:\", reference)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    bleu = corpus_bleu(translations, references)\n",
    "    print(f\"\\nBLEU score: {bleu.score}\")\n",
    "    \n",
    "    # Calculate COMET score\n",
    "    comet = evaluate.load('comet')\n",
    "    comet_scores = comet.compute(\n",
    "        predictions=translations,\n",
    "        references=[ref[0] for ref in references],\n",
    "        sources=sources  # Use stored source texts\n",
    "    )\n",
    "    print(f\"COMET score: {comet_scores['mean_score']}\")\n",
    "    \n",
    "    return {\n",
    "        'translations': translations,\n",
    "        'references': references,\n",
    "        'bleu': bleu.score,\n",
    "        'comet': comet_scores['mean_score']\n",
    "    }\n",
    "\n",
    "# Clear cache before evaluation\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Get baseline results\n",
    "baseline_results = evaluate_translations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f80f8cf8-98d8-443f-8820-9d3c8ac76bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT training data (first 5000 examples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef5c19ee1634586ab3d45eee810a3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying training data format:\n",
      "\n",
      "Example 0:\n",
      "Input: Translate from German to English:\n",
      "2 Personen *** 3 Personen\n",
      "Translation: 2 persons *** 3 persons</s>\n",
      "Output: 2 persons *** 3 persons\n",
      "\n",
      "Example 1:\n",
      "Input: Translate from German to English:\n",
      "Hintergrund :\n",
      "Translation: Background :</s>\n",
      "Output: Background :\n",
      "\n",
      "Example 2:\n",
      "Input: Translate from German to English:\n",
      "Für ELA-Tonsäulen mit M6-Gewindebuchse wie z.B. ETS-215TW/WS, ETS-210TW/WS oder ETS-215/WS\n",
      "Translation: For PA column speakers with M6 threaded bushing, e.g. ETS-215TW/WS, ETS-210TW/WS or ETS-215/WS</s>\n",
      "Output: For PA column speakers with M6 threaded bushing, e.g. ETS-215TW/WS, ETS-210TW/WS or ETS-215/WS\n",
      "\n",
      "Prepared 5000 examples for training\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Prepare WMT Training Data\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "def prepare_wmt_training_data(num_examples=5000):\n",
    "    print(f\"Loading WMT training data (first {num_examples} examples)...\")\n",
    "    \n",
    "    train_data = load_dataset(\"wmt19\", \"de-en\", split=\"train\")\n",
    "    train_data = train_data.shuffle(seed=42).select(range(num_examples))\n",
    "    \n",
    "    def format_for_training(examples):\n",
    "        formatted_inputs = []\n",
    "        formatted_outputs = []\n",
    "        \n",
    "        for item in examples['translation']:\n",
    "            source = item['de']\n",
    "            target = item['en']\n",
    "            \n",
    "            # Format input with prompt\n",
    "            input_text = f\"Translate from German to English:\\n{source}\\nTranslation: {target}</s>\"\n",
    "            formatted_inputs.append(input_text)\n",
    "            formatted_outputs.append(target)\n",
    "        \n",
    "        return {\n",
    "            'inputs': formatted_inputs,\n",
    "            'outputs': formatted_outputs\n",
    "        }\n",
    "    \n",
    "    formatted_data = train_data.map(\n",
    "        format_for_training, \n",
    "        batched=True, \n",
    "        remove_columns=train_data.column_names\n",
    "    )\n",
    "    \n",
    "    # Print some examples to verify format\n",
    "    print(\"\\nVerifying training data format:\")\n",
    "    for i in range(3):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(\"Input:\", formatted_data[i]['inputs'])\n",
    "        print(\"Output:\", formatted_data[i]['outputs'])\n",
    "    \n",
    "    print(f\"\\nPrepared {len(formatted_data)} examples for training\")\n",
    "    return formatted_data\n",
    "\n",
    "# Prepare training data\n",
    "training_data = prepare_wmt_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1347b979-b60f-4513-ab9d-c59e0fffafa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model with LoRA configuration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a93102de4d641c287dac093c60c4636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model configuration:\n",
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Setup LoRA Configuration and Model for Training\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def setup_model_for_training():\n",
    "    print(\"Setting up model with LoRA configuration...\")\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,  # Rank\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model_for_training = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    # Freeze embedding layer\n",
    "    for param in model_for_training.get_input_embeddings().parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Add LoRA adapters\n",
    "    model_for_training = get_peft_model(model_for_training, lora_config)\n",
    "    \n",
    "    # Print trainable parameters info\n",
    "    print(\"\\nModel configuration:\")\n",
    "    model_for_training.print_trainable_parameters()\n",
    "    \n",
    "    return model_for_training\n",
    "\n",
    "# Setup model\n",
    "model_for_training = setup_model_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08401357-6d1a-4c1a-b322-7a284f5feabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    logging_steps=20,\n",
    "    weight_decay=0.01,\n",
    "    max_steps=500,\n",
    "    warmup_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9a2f82f-1e06-495f-b456-63a5b91f92dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125a88c1ab8848fd8e68f24b14afe164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing data:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 9: Prepare Data Collator and Tokenize Training Data\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def prepare_training_data():\n",
    "    print(\"Preparing training data...\")\n",
    "    \n",
    "    def tokenize_data(examples):\n",
    "        model_inputs = tokenizer(\n",
    "            examples['inputs'],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        \n",
    "        # Set labels to be the same as inputs for causal LM training\n",
    "        model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "        \n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_data = training_data.map(\n",
    "        tokenize_data,\n",
    "        batched=True,\n",
    "        remove_columns=training_data.column_names,\n",
    "        desc=\"Tokenizing data\"\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    return tokenized_data, data_collator\n",
    "\n",
    "# Prepare training data and collator\n",
    "tokenized_training_data, data_collator = prepare_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecfb5e5c-e97b-46a4-a62a-e60373ac0817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 10: Start Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m----> 4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_for_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_training_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    607\u001b[0m ):\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:881\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 881\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1333\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1334\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen moving module from meta to a different device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1336\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "# Cell 10: Start Training\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd874c-7338-4e2b-a190-6141c99ed7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Debug Evaluation\n",
    "def evaluate_finetuned_model(num_examples=5, num_to_show=5):\n",
    "    print(f\"Debugging fine-tuned model on {num_examples} examples...\")\n",
    "    \n",
    "    eval_subset = eval_dataset.select(range(num_examples))\n",
    "    translations = []\n",
    "    references = []\n",
    "    \n",
    "    for i, example in enumerate(eval_subset):\n",
    "        source_text = example['translation']['de']\n",
    "        reference = example['translation']['en']\n",
    "        \n",
    "        # Print exact prompt being used\n",
    "        prompt = f\"Translate from German to English:\\n{source_text}\\nTranslation:\"\n",
    "        print(f\"\\nExample {i} - Full Prompt:\")\n",
    "        print(\"=\"*50)\n",
    "        print(prompt)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model_for_training.device)\n",
    "        \n",
    "        # Print tokenized input\n",
    "        print(\"\\nTokenized input:\")\n",
    "        print(tokenizer.decode(inputs['input_ids'][0]))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_for_training.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True  # Get token scores\n",
    "            )\n",
    "        \n",
    "        # Get full output including intermediate tokens\n",
    "        full_output = tokenizer.decode(outputs.sequences[0], skip_special_tokens=False)\n",
    "        translation = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "        translation = translation.split(\"Translation:\")[-1].strip()\n",
    "        \n",
    "        print(\"\\nFull model output (including special tokens):\")\n",
    "        print(full_output)\n",
    "        print(\"\\nExtracted translation:\")\n",
    "        print(translation)\n",
    "        print(\"\\nReference:\")\n",
    "        print(reference)\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        translations.append(translation)\n",
    "        references.append([reference])\n",
    "    \n",
    "    bleu = corpus_bleu(translations, references)\n",
    "    print(f\"\\nBLEU score: {bleu.score}\")\n",
    "    \n",
    "    comet = evaluate.load('comet')\n",
    "    comet_scores = comet.compute(\n",
    "        predictions=translations,\n",
    "        references=[ref[0] for ref in references],\n",
    "        sources=[ex['translation']['de'] for ex in eval_subset]\n",
    "    )\n",
    "    print(f\"COMET score: {comet_scores['mean_score']}\")\n",
    "    \n",
    "    return {\n",
    "        'translations': translations,\n",
    "        'references': references,\n",
    "        'bleu': bleu.score,\n",
    "        'comet': comet_scores['mean_score']\n",
    "    }\n",
    "\n",
    "# Run debug evaluation\n",
    "debug_results = evaluate_finetuned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720933a-dc9e-43d2-9d12-45783f2f1218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c1ef1-674c-48e9-a126-ffe0d4c36b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a242c44-48f2-42f8-bbc7-1db80cbd82f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
