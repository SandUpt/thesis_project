{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b624eade-f0ef-4678-b58b-ddb106cced58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.2\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 1: Initial imports and environment setup\n",
    "# --------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./my_results\", exist_ok=True)\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73153aae-bf4c-4706-8b91-402d38f6bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 2: Evaluation Function Definition (Need this early)\n",
    "# --------------------------------------------\n",
    "def build_prompt_for_translation(german_text: str) -> str:\n",
    "    prompt = (\n",
    "        \"Translate this German text into fluent, natural English:\\n\"\n",
    "        f\"German: {german_text}\\n\"\n",
    "        \"English:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def debug_evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"Model\"\n",
    "):\n",
    "    comet_metric = evaluate.load(\"comet\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    sources = []\n",
    "\n",
    "    subset = eval_dataset.select(range(min(num_examples, len(eval_dataset))))\n",
    "    print(f\"\\n[DEBUG EVAL] {description} on {num_examples} examples...\\n\")\n",
    "\n",
    "    for i, ex in enumerate(tqdm(subset, desc=f\"Evaluating {description}\")):\n",
    "        src_de = ex[\"translation\"][\"de\"]\n",
    "        ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "        prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "        tokenized_input = tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **tokenized_input,\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "\n",
    "        if \"English:\" in full_output_text:\n",
    "            pred_en = full_output_text.rsplit(\"English:\", 1)[-1].strip()\n",
    "        else:\n",
    "            pred_en = full_output_text\n",
    "\n",
    "        predictions.append(pred_en)\n",
    "        references.append([ref_en])\n",
    "        sources.append(src_de)\n",
    "\n",
    "        if i < debug_print:\n",
    "            print(\"\\n==========================================\")\n",
    "            print(f\"Example {i}\")\n",
    "            print(\"---------------[ PROMPT ]-----------------\")\n",
    "            print(prompt_text)\n",
    "            print(\"--------------[ TOKENIZED ]---------------\")\n",
    "            print(f\"Input IDs: {tokenized_input['input_ids'][0].tolist()}\")\n",
    "            print(\"-----------[ FULL MODEL OUTPUT ]----------\")\n",
    "            print(repr(full_output_text))\n",
    "            print(\"-------------[ EXTRACTED EN ]-------------\")\n",
    "            print(repr(pred_en))\n",
    "            print(\"--------------[ REFERENCE ]---------------\")\n",
    "            print(ref_en)\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "    bleu = corpus_bleu(predictions, references)\n",
    "    print(f\"[{description}] BLEU = {bleu.score:.2f}\")\n",
    "\n",
    "    comet_results = comet_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=[r[0] for r in references],\n",
    "        sources=sources\n",
    "    )\n",
    "    print(f\"[{description}] COMET = {comet_results['mean_score']:.3f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"bleu\": bleu.score,\n",
    "        \"comet\": comet_results[\"mean_score\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce09c1f-6e51-43e3-bde7-db4ab7af4ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc32e7e24ba40569a51b25f3c9666b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model and tokenizer ready.\n",
      "Loading WMT19 (de-en) validation data with 50 examples...\n",
      "Loading saved baseline results...\n",
      "Baseline -> BLEU = 48.89, COMET = 0.542\n",
      "Baseline evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 3: Load/Save Base Model & Get Baseline Scores\n",
    "# --------------------------------------------\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_save_path = \"./saved_models/base_llama\"\n",
    "\n",
    "def load_or_download_base_model():\n",
    "    if os.path.exists(base_model_save_path):\n",
    "        print(\"Loading saved base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_save_path)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_save_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        print(\"Downloading base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        # Save the model and tokenizer\n",
    "        print(\"Saving base model...\")\n",
    "        tokenizer.save_pretrained(base_model_save_path)\n",
    "        base_model.save_pretrained(base_model_save_path)\n",
    "        print(f\"Base model saved to {base_model_save_path}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer, base_model\n",
    "\n",
    "def load_eval_data(num_examples=50):\n",
    "    print(f\"Loading WMT19 (de-en) validation data with {num_examples} examples...\")\n",
    "    eval_data = load_dataset(\"wmt19\", \"de-en\", split=\"validation\")\n",
    "    eval_data = eval_data.select(range(min(num_examples, len(eval_data))))\n",
    "    return eval_data\n",
    "\n",
    "# Load/download the model\n",
    "tokenizer, base_model = load_or_download_base_model()\n",
    "print(\"Base model and tokenizer ready.\")\n",
    "\n",
    "# Get baseline scores immediately\n",
    "eval_dataset = load_eval_data(num_examples=50)\n",
    "baseline_results_path = \"./results/baseline_results.json\"\n",
    "\n",
    "if os.path.exists(baseline_results_path):\n",
    "    print(\"Loading saved baseline results...\")\n",
    "    with open(baseline_results_path, 'r') as f:\n",
    "        baseline_debug_results = json.load(f)\n",
    "    print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "else:\n",
    "    print(\"Evaluating baseline model...\")\n",
    "    baseline_debug_results = debug_evaluate_model(\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        eval_dataset=eval_dataset,\n",
    "        num_examples=20,\n",
    "        debug_print=3,\n",
    "        description=\"Baseline LLaMA\"\n",
    "    )\n",
    "    # Save results\n",
    "    with open(baseline_results_path, 'w') as f:\n",
    "        json.dump(baseline_debug_results, f)\n",
    "\n",
    "print(\"Baseline evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9452c598-3393-4026-9938-bd2dd3eadfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT19 (de-en) train data with 10000 examples...\n",
      "Training data prepared with prompt masking.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 4: Prepare Training Data\n",
    "# --------------------------------------------\n",
    "def build_full_text(example):\n",
    "    german = example[\"translation\"][\"de\"]\n",
    "    english = example[\"translation\"][\"en\"]\n",
    "    prompt = (\n",
    "        \"Translate this German text into fluent English.\\n\"\n",
    "        f\"{german}\\n\"\n",
    "        \"Translation:\"\n",
    "    )\n",
    "    full_text = prompt + \" \" + english\n",
    "    return {\"full_text\": full_text}\n",
    "\n",
    "def load_and_format_wmt(num_examples=10000):\n",
    "    print(f\"Loading WMT19 (de-en) train data with {num_examples} examples...\")\n",
    "    dataset = load_dataset(\"wmt19\", \"de-en\", split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42).select(range(num_examples))\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        build_full_text,\n",
    "        desc=\"Building prompt + target text\",\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "class PromptMaskCollator:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        texts = [ex[\"full_text\"] for ex in examples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if \"Translation:\" in text:\n",
    "                prompt_part, _ = text.split(\"Translation:\", 1)\n",
    "                prompt_part = prompt_part + \"Translation:\"\n",
    "            else:\n",
    "                prompt_part = text\n",
    "            \n",
    "            prompt_ids = self.tokenizer(\n",
    "                prompt_part,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            prompt_len = len(prompt_ids)\n",
    "            if prompt_len > labels.size(1):\n",
    "                prompt_len = labels.size(1)\n",
    "            \n",
    "            labels[i, :prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Load training data\n",
    "training_data = load_and_format_wmt(num_examples=10000)\n",
    "data_collator = PromptMaskCollator(tokenizer, max_length=512)\n",
    "\n",
    "print(\"Training data prepared with prompt masking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93afa863-4e0e-43bb-bb3a-a8e16e93c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LoRA model...\n",
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484\n",
      "LoRA model is ready.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 5: LoRA Configuration and Model Setup\n",
    "# --------------------------------------------\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def setup_lora_model():\n",
    "    print(\"Setting up LoRA model...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(base_model, lora_config)  # Use existing base_model\n",
    "    lora_model.print_trainable_parameters()\n",
    "    return lora_model\n",
    "\n",
    "model_for_training = setup_lora_model()\n",
    "print(\"LoRA model is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7263710-7008-4c6e-9bac-80419f38d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments set. Will train for 2 epochs on ~10000 examples.\n",
      "Starting LoRA fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [624/624 42:14, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished.\n",
      "Training metrics:\n",
      "TrainOutput(global_step=624, training_loss=0.9375207231212885, metrics={'train_runtime': 2539.4354, 'train_samples_per_second': 7.876, 'train_steps_per_second': 0.246, 'total_flos': 4.060092175566766e+17, 'train_loss': 0.9375207231212885, 'epoch': 1.9952})\n",
      "Fine-tuning done. Model saved at ./my_results/lora_7b\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 6: Training Setup and Execution\n",
    "# --------------------------------------------\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./my_results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    ddp_find_unused_parameters=False  # Added this\n",
    ")\n",
    "\n",
    "print(f\"Training arguments set. Will train for {train_args.num_train_epochs} epochs on ~{len(training_data)} examples.\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=train_args,\n",
    "    train_dataset=training_data,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "train_output = trainer.train()\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "print(train_output)\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./my_results/lora_7b\")\n",
    "print(\"Fine-tuning done. Model saved at ./my_results/lora_7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba704a9-af91-4fe7-87e2-e8563dd04290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating LoRA-Fine-Tuned Model...\n",
      "Loading LoRA model from ./my_results/lora_7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16eb6e209a8448b59d5b271c027d4f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a55bb06fe249619f7551a6823a29ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG EVAL] LoRA Fine-Tuned on 20 examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned:   0%|                                                              | 0/20 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating LoRA Fine-Tuned:   5%|██▋                                                   | 1/20 [00:00<00:13,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 0\n",
      "---------------[ PROMPT ]-----------------\n",
      "Translate this German text into fluent, natural English:\n",
      "German: München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\n",
      "English:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 4103, 9632, 445, 5332, 1426, 964, 1652, 8122, 29892, 5613, 4223, 29901, 13, 29954, 3504, 29901, 10864, 29871, 29896, 29947, 29945, 29953, 29901, 23650, 476, 8109, 29892, 762, 306, 13608, 350, 1406, 1622, 762, 5587, 1147, 3140, 824, 13, 24636, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'<s> Translate this German text into fluent, natural English:\\nGerman: München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\\nEnglish: Munich 1856: Four cards that will change your view of the city</s>'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Munich 1856: Four cards that will change your view of the city</s>'\n",
      "--------------[ REFERENCE ]---------------\n",
      "Munich 1856: Four maps that will change your view of the city\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned:  10%|█████▍                                                | 2/20 [00:01<00:09,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 1\n",
      "---------------[ PROMPT ]-----------------\n",
      "Translate this German text into fluent, natural English:\n",
      "German: Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\n",
      "English:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 4103, 9632, 445, 5332, 1426, 964, 1652, 8122, 29892, 5613, 4223, 29901, 13, 29954, 3504, 29901, 11281, 6600, 1267, 29899, 2744, 303, 1997, 29892, 8879, 2160, 12843, 19472, 4545, 1812, 387, 4566, 899, 2435, 29889, 13, 24636, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "\"<s> Translate this German text into fluent, natural English:\\nGerman: Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\\nEnglish: A lunatic asylum, where today's youth should meet.</s>\"\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "\"A lunatic asylum, where today's youth should meet.</s>\"\n",
      "--------------[ REFERENCE ]---------------\n",
      "A mental asylum, where today young people are said to meet.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned:  15%|████████                                              | 3/20 [00:01<00:08,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 2\n",
      "---------------[ PROMPT ]-----------------\n",
      "Translate this German text into fluent, natural English:\n",
      "German: Eine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\n",
      "English:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 4103, 9632, 445, 5332, 1426, 964, 1652, 8122, 29892, 5613, 4223, 29901, 13, 29954, 3504, 29901, 11281, 5430, 615, 21474, 1808, 29892, 8879, 11923, 1865, 972, 317, 29899, 29933, 5422, 29899, 29911, 16163, 21598, 336, 1785, 4296, 29889, 13, 24636, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'<s> Translate this German text into fluent, natural English:\\nGerman: Eine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\\nEnglish: A graveyard chapel, where now the S-Bahn tunnel is being dug.</s>'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'A graveyard chapel, where now the S-Bahn tunnel is being dug.</s>'\n",
      "--------------[ REFERENCE ]---------------\n",
      "A crypt chapel, where they are now digging tunnels for the S-Bahn.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned: 100%|█████████████████████████████████████████████████████| 20/20 [00:12<00:00,  1.64it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA Fine-Tuned] BLEU = 57.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA Fine-Tuned] COMET = 0.792\n",
      "\n",
      "\n",
      "Final comparison:\n",
      "Baseline -> BLEU = 48.89, COMET = 0.542\n",
      "LoRA     -> BLEU = 57.49, COMET = 0.792\n",
      "LoRA results saved to ./results/lora_results.json\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 7: Evaluate Fine-tuned Model\n",
    "# --------------------------------------------\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_lora_model(checkpoint_path=\"./my_results/lora_7b\"):\n",
    "    print(f\"Loading LoRA model from {checkpoint_path}...\")\n",
    "    # Load saved base model\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_save_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Merge LoRA\n",
    "    lora_model_loaded = PeftModel.from_pretrained(base, checkpoint_path)\n",
    "    lora_model_loaded = lora_model_loaded.merge_and_unload()\n",
    "    return lora_model_loaded\n",
    "\n",
    "print(\"\\nEvaluating LoRA-Fine-Tuned Model...\")\n",
    "merged_model = load_lora_model(\"./my_results/lora_7b\")\n",
    "lora_debug_results = debug_evaluate_model(\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"LoRA Fine-Tuned\"\n",
    ")\n",
    "\n",
    "print(\"\\nFinal comparison:\")\n",
    "print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "print(f\"LoRA     -> BLEU = {lora_debug_results['bleu']:.2f}, COMET = {lora_debug_results['comet']:.3f}\")\n",
    "\n",
    "# Save LoRA results\n",
    "lora_results_path = \"./results/lora_results.json\"\n",
    "with open(lora_results_path, 'w') as f:\n",
    "    json.dump(lora_debug_results, f)\n",
    "print(f\"LoRA results saved to {lora_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87d621-fd30-4b2a-ac69-e071c8963312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c67e3-b718-42fd-83cb-b89dcfcc8131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
