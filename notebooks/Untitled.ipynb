{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9256234d-8f6d-41bf-82de-37f562f1007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.2\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 1: Initial imports and environment setup\n",
    "# --------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# (Optional) Add your Hugging Face token if needed\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507405b0-343e-4218-b674-a0e2e1e5708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading base model in float16 with device_map='auto'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ea488322184368ac8bf2029a71ce8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 2: Load Base Model & Tokenizer (Baseline)\n",
    "# --------------------------------------------\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading base model in float16 with device_map='auto'...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Ensure a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Baseline model and tokenizer loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c47fbe1-43fa-4ebc-8f62-c247383cd8dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT19 (de-en) train data with 50000 examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec973c2eb8e49a796d3878acdecae9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building prompt + target text:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data prepared with prompt masking.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 3: Prepare WMT19 Data (Prompt Masking)\n",
    "# --------------------------------------------\n",
    "def build_full_text(example):\n",
    "    german = example[\"translation\"][\"de\"]\n",
    "    english = example[\"translation\"][\"en\"]\n",
    "    prompt = (\n",
    "        \"Translate this German text into fluent English.\\n\"\n",
    "        f\"{german}\\n\"\n",
    "        \"Translation:\"\n",
    "    )\n",
    "    full_text = prompt + \" \" + english\n",
    "    return {\"full_text\": full_text}\n",
    "\n",
    "def load_and_format_wmt(num_examples=50000):\n",
    "    print(f\"Loading WMT19 (de-en) train data with {num_examples} examples...\")\n",
    "    dataset = load_dataset(\"wmt19\", \"de-en\", split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42).select(range(num_examples))\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        build_full_text,\n",
    "        desc=\"Building prompt + target text\",\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "class PromptMaskCollator:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        texts = [ex[\"full_text\"] for ex in examples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if \"Translation:\" in text:\n",
    "                prompt_part, _ = text.split(\"Translation:\", 1)\n",
    "                prompt_part = prompt_part + \"Translation:\"\n",
    "            else:\n",
    "                prompt_part = text\n",
    "            \n",
    "            prompt_ids = self.tokenizer(\n",
    "                prompt_part,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            prompt_len = len(prompt_ids)\n",
    "            if prompt_len > labels.size(1):\n",
    "                prompt_len = labels.size(1)\n",
    "            \n",
    "            labels[i, :prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Load training data for fine-tuning\n",
    "training_data = load_and_format_wmt(num_examples=50000)\n",
    "data_collator = PromptMaskCollator(tokenizer, max_length=512)\n",
    "\n",
    "print(\"Training data prepared with prompt masking.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d2da5c-d58c-4c25-a730-98f0a96c4cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LoRA model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b99f94109c45569414177e6f90175e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n",
      "LoRA model is ready.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 4: LoRA Configuration and Model\n",
    "# --------------------------------------------\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def setup_lora_model():\n",
    "    print(\"Setting up LoRA model...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    lora_base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Freeze embeddings\n",
    "    for param in lora_base.get_input_embeddings().parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(lora_base, lora_config)\n",
    "    lora_model.print_trainable_parameters()\n",
    "    return lora_model\n",
    "\n",
    "model_for_training = setup_lora_model()\n",
    "\n",
    "print(\"LoRA model is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087b8668-f9ae-4508-ac1e-1f0f8eff75a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments set.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 5: Training Arguments\n",
    "# --------------------------------------------\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,  # 1 epoch over 50k lines\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    # Crucial to avoid the \"no columns match\" error:\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "print(\"Training arguments set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30dd9f46-cc1e-47cd-b318-96fd43bdf408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA fine-tuning...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1562' max='1562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1562/1562 1:57:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>29.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished.\n",
      "Training metrics:\n",
      "TrainOutput(global_step=1562, training_loss=2.0626527741074407, metrics={'train_runtime': 7031.6059, 'train_samples_per_second': 7.111, 'train_steps_per_second': 0.222, 'total_flos': 1.015851293136126e+18, 'train_loss': 2.0626527741074407, 'epoch': 0.99968})\n",
      "Fine-tuning done.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 6: Initialize Trainer & Start Fine-tuning\n",
    "# --------------------------------------------\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=train_args,\n",
    "    train_dataset=training_data,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\\n\")\n",
    "train_output = trainer.train()\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "print(train_output)\n",
    "\n",
    "print(\"Fine-tuning done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e6a0fd-fd3c-4cec-b871-6f4f2fc59444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT19 (de-en) validation data with 100 examples...\n",
      "Validation data loaded.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 7: Load Evaluation Data\n",
    "# --------------------------------------------\n",
    "def load_eval_data(num_examples=100):\n",
    "    print(f\"Loading WMT19 (de-en) validation data with {num_examples} examples...\")\n",
    "    eval_data = load_dataset(\"wmt19\", \"de-en\", split=\"validation\")\n",
    "    eval_data = eval_data.select(range(min(num_examples, len(eval_data))))\n",
    "    return eval_data\n",
    "\n",
    "eval_dataset = load_eval_data(num_examples=100)\n",
    "\n",
    "print(\"Validation data loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83ed19c-f6be-4731-bab4-1182c72c7eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the *baseline* model...\n",
      "\n",
      "Evaluating Baseline on 20 examples. Showing 3...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline:   0%|                                                                       | 0/20 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating Baseline:   5%|███▏                                                           | 1/20 [00:06<01:54,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "  Source (DE): München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\n",
      "  Predicted (EN): Munich 1856: Four cards that will change the way you look at the city\n",
      "München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern.\n",
      "Munich 1856: Four cards that will change the way you look at the city.\n",
      "München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern. München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern. München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern. München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern. München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern. München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern. München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern. München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern.\n",
      "  Reference (EN): Munich 1856: Four maps that will change your view of the city\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline:  10%|██████▎                                                        | 2/20 [00:07<01:03,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "  Source (DE): Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\n",
      "  Predicted (EN): An insane asylum, where today's youth should meet.\n",
      "  Reference (EN): A mental asylum, where today young people are said to meet.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline:  15%|█████████▍                                                     | 3/20 [00:09<00:43,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2\n",
      "  Source (DE): Eine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\n",
      "  Predicted (EN): A graveyard chapel, where now the S-Bahn tunnel is being dug.\n",
      "  Reference (EN): A crypt chapel, where they are now digging tunnels for the S-Bahn.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline: 100%|██████████████████████████████████████████████████████████████| 20/20 [01:05<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Baseline] BLEU score: 2.689745497366639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b769f0f52cae4bceb817c3ee4013afaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] COMET score: 0.649969045817852\n",
      "Baseline evaluation done.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 8: Universal Evaluation Function & Baseline Eval\n",
    "# --------------------------------------------\n",
    "def evaluate_model(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    eval_dataset, \n",
    "    num_examples=20, \n",
    "    num_to_show=5, \n",
    "    description=\"Model\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate any given `model` on `eval_dataset`.\n",
    "    Returns BLEU & COMET. Prints a few translations.\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {description} on {num_examples} examples. Showing {num_to_show}...\\n\")\n",
    "    \n",
    "    subset = eval_dataset.select(range(num_examples))\n",
    "    translations = []\n",
    "    references = []\n",
    "    \n",
    "    for i, ex in enumerate(tqdm(subset, desc=f\"Evaluating {description}\")):\n",
    "        src_de = ex[\"translation\"][\"de\"]\n",
    "        ref_en = ex[\"translation\"][\"en\"]\n",
    "        \n",
    "        prompt = (\n",
    "            \"Translate this German text into fluent English.\\n\"\n",
    "            f\"{src_de}\\n\"\n",
    "            \"Translation:\"\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        if \"Translation:\" in full_output:\n",
    "            pred_en = full_output.split(\"Translation:\")[-1].strip()\n",
    "        else:\n",
    "            pred_en = full_output\n",
    "        \n",
    "        translations.append(pred_en)\n",
    "        references.append([ref_en])\n",
    "        \n",
    "        if i < num_to_show:\n",
    "            print(f\"Example {i}\")\n",
    "            print(\"  Source (DE):\", src_de)\n",
    "            print(\"  Predicted (EN):\", pred_en)\n",
    "            print(\"  Reference (EN):\", ref_en)\n",
    "            print(\"-\"*80)\n",
    "    \n",
    "    # Calculate BLEU\n",
    "    bleu = corpus_bleu(translations, references)\n",
    "    print(f\"\\n[{description}] BLEU score: {bleu.score}\")\n",
    "\n",
    "    # Calculate COMET\n",
    "    comet_metric = evaluate.load(\"comet\")\n",
    "    comet_results = comet_metric.compute(\n",
    "        predictions=translations,\n",
    "        references=[r[0] for r in references],\n",
    "        sources=[ex[\"translation\"][\"de\"] for ex in subset]\n",
    "    )\n",
    "    print(f\"[{description}] COMET score: {comet_results['mean_score']}\")\n",
    "    \n",
    "    return {\n",
    "        \"translations\": translations,\n",
    "        \"references\": references,\n",
    "        \"bleu\": bleu.score,\n",
    "        \"comet\": comet_results[\"mean_score\"]\n",
    "    }\n",
    "\n",
    "# ---- Evaluate Baseline Model ----\n",
    "print(\"Evaluating the *baseline* model...\\n\")\n",
    "baseline_results = evaluate_model(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset, \n",
    "    num_examples=20,   # evaluate on 20 for quick test\n",
    "    num_to_show=3,\n",
    "    description=\"Baseline\"\n",
    ")\n",
    "\n",
    "print(\"Baseline evaluation done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af4b295-faf1-433b-bbfb-aaebe06395e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the *fine-tuned* model...\n",
      "\n",
      "Evaluating Fine-tuned on 20 examples. Showing 3...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Fine-tuned:   0%|                                                                     | 0/20 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating Fine-tuned:   5%|███                                                          | 1/20 [00:00<00:14,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "  Source (DE): München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\n",
      "  Predicted (EN): Munich 1856: Four cards that change your view of the city\n",
      "  Reference (EN): Munich 1856: Four maps that will change your view of the city\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Fine-tuned:  10%|██████                                                       | 2/20 [00:01<00:13,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "  Source (DE): Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\n",
      "  Predicted (EN): A lunatic asylum, where today's youth should meet.\n",
      "  Reference (EN): A mental asylum, where today young people are said to meet.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Fine-tuned:  15%|█████████▏                                                   | 3/20 [00:02<00:13,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2\n",
      "  Source (DE): Eine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\n",
      "  Predicted (EN): A crypt chapel, where now the S-Bahn tunnel is being dug.\n",
      "  Reference (EN): A crypt chapel, where they are now digging tunnels for the S-Bahn.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Fine-tuned: 100%|████████████████████████████████████████████████████████████| 20/20 [00:20<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fine-tuned] BLEU score: 64.93358309501976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d4351c13324ce897d6aabae6c08eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fine-tuned] COMET score: 0.8147880345582962\n",
      "\n",
      "Comparison:\n",
      "Baseline BLEU: 2.69 | COMET: 0.650\n",
      "Fine-tuned BLEU: 64.93 | COMET: 0.815\n",
      "\n",
      "[Cell 9 complete] Fine-tuned model evaluation & comparison finished.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 9: Evaluate Fine-Tuned Model & Compare\n",
    "# --------------------------------------------\n",
    "print(\"Evaluating the *fine-tuned* model...\\n\")\n",
    "finetuned_results = evaluate_model(\n",
    "    model=model_for_training,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_examples=20,\n",
    "    num_to_show=3,\n",
    "    description=\"Fine-tuned\"\n",
    ")\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Baseline BLEU: {baseline_results['bleu']:.2f} | COMET: {baseline_results['comet']:.3f}\")\n",
    "print(f\"Fine-tuned BLEU: {finetuned_results['bleu']:.2f} | COMET: {finetuned_results['comet']:.3f}\")\n",
    "\n",
    "print(\"Fine-tuned model evaluation & comparison finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442076dc-e5e6-4644-9d94-efcb4e4b6bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdac789-37dd-4d2c-b6e4-f95cb6a9b34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
