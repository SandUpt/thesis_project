{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624eade-f0ef-4678-b58b-ddb106cced58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate# CELL 1\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./my_results\", exist_ok=True)\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./my_results\", exist_ok=True)\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73153aae-bf4c-4706-8b91-402d38f6bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_save_path = \"./saved_models/base_llama\"\n",
    "\n",
    "def load_or_download_base_model():\n",
    "    if os.path.exists(base_model_save_path):\n",
    "        print(\"Loading saved base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_save_path)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_save_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        print(\"Downloading base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        print(\"Saving base model...\")\n",
    "        tokenizer.save_pretrained(base_model_save_path)\n",
    "        base_model.save_pretrained(base_model_save_path)\n",
    "        print(f\"Base model saved to {base_model_save_path}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer, base_model\n",
    "\n",
    "tokenizer, base_model = load_or_download_base_model()\n",
    "print(\"Base model and tokenizer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce09c1f-6e51-43e3-bde7-db4ab7af4ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CELL 3\n",
    "\n",
    "import re\n",
    "\n",
    "###############################\n",
    "# 1) Helper: parse_sgm_to_list\n",
    "###############################\n",
    "def parse_sgm_to_list(sgm_file):\n",
    "    \"\"\"\n",
    "    Reads an SGM file (e.g. newstest2018-deen-src.de.sgm)\n",
    "    and returns a list of lines, one per <seg id=\\\"...\\\"> block.\n",
    "    \"\"\"\n",
    "    with open(sgm_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    # Capture everything between <seg id=\\\"X\\\"> and </seg>, allowing multiline\n",
    "    segments = re.findall(r'<seg id=\\\"\\d+\\\">(.*?)</seg>', content, flags=re.DOTALL)\n",
    "    segments = [seg.strip() for seg in segments]\n",
    "    return segments\n",
    "\n",
    "###############################\n",
    "# 2) Helper: load_sgm_parallel\n",
    "###############################\n",
    "def load_sgm_parallel(de_sgm_file, en_sgm_file):\n",
    "    \"\"\"\n",
    "    Pairs up lines from a German .sgm file and an English .sgm file\n",
    "    into a list of dicts: {\\\"translation\\\": {\\\"de\\\": ..., \\\"en\\\": ...}}.\n",
    "    \"\"\"\n",
    "    de_lines = parse_sgm_to_list(de_sgm_file)\n",
    "    en_lines = parse_sgm_to_list(en_sgm_file)\n",
    "    assert len(de_lines) == len(en_lines), (\n",
    "        f\"Mismatch in line counts: {len(de_lines)} vs {len(en_lines)}\"\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "    for de_text, en_text in zip(de_lines, en_lines):\n",
    "        data.append({\n",
    "            \"translation\": {\n",
    "                \"de\": de_text,\n",
    "                \"en\": en_text\n",
    "            }\n",
    "        })\n",
    "    return data\n",
    "\n",
    "###############################\n",
    "# 3) Load dev data locally\n",
    "###############################\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_dev_data_local(num_examples=50):\n",
    "    \"\"\"\n",
    "    Loads newstest2018-deen from local .sgm files in wmt_dataset/dev/.\n",
    "    Truncates to num_examples if desired.\n",
    "    \"\"\"\n",
    "    print(f\"Loading local dev set (2018) with up to {num_examples} examples...\")\n",
    "\n",
    "    dev_de_path = \"wmt_dataset/dev/newstest2018-deen-src.de.sgm\"\n",
    "    dev_en_path = \"wmt_dataset/dev/newstest2018-deen-ref.en.sgm\"\n",
    "\n",
    "    dev_data_list = load_sgm_parallel(dev_de_path, dev_en_path)\n",
    "    dev_data_list = dev_data_list[:num_examples]  # slice if you only want n lines\n",
    "\n",
    "    dev_dataset = Dataset.from_list(dev_data_list)\n",
    "    return dev_dataset\n",
    "\n",
    "###############################\n",
    "# 4) keep the build_prompt & debug_evaluate_model\n",
    "###############################\n",
    "def build_prompt_for_translation(german_text: str) -> str:\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german_text}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def debug_evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"Model\"\n",
    "):\n",
    "    comet_metric = evaluate.load(\"comet\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    sources = []\n",
    "\n",
    "    subset = eval_dataset.select(range(min(num_examples, len(eval_dataset))))\n",
    "    print(f\"\\n[DEBUG EVAL] {description} on {num_examples} examples...\\n\")\n",
    "\n",
    "    for i, ex in enumerate(tqdm(subset, desc=f\"Evaluating {description}\")):\n",
    "        src_de = ex[\"translation\"][\"de\"]\n",
    "        ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "        prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "        tokenized_input = tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **tokenized_input,\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"English translation:\" in full_output_text:\n",
    "            pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "        else:\n",
    "            pred_en = full_output_text.split(\"German text:\")[-1].strip()\n",
    "\n",
    "        predictions.append(pred_en)\n",
    "        references.append([ref_en])\n",
    "        sources.append(src_de)\n",
    "\n",
    "        if i < debug_print:\n",
    "            print(\"\\n==========================================\")\n",
    "            print(f\"Example {i}\")\n",
    "            print(\"---------------[ PROMPT ]-----------------\")\n",
    "            print(prompt_text)\n",
    "            print(\"--------------[ TOKENIZED ]---------------\")\n",
    "            print(f\"Input IDs: {tokenized_input['input_ids'][0].tolist()}\")\n",
    "            print(\"-----------[ FULL MODEL OUTPUT ]----------\")\n",
    "            print(repr(full_output_text))\n",
    "            print(\"-------------[ EXTRACTED EN ]-------------\")\n",
    "            print(repr(pred_en))\n",
    "            print(\"--------------[ REFERENCE ]---------------\")\n",
    "            print(ref_en)\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "    bleu = corpus_bleu(predictions, references)\n",
    "    print(f\"[{description}] BLEU = {bleu.score:.2f}\")\n",
    "\n",
    "    comet_results = comet_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=[r[0] for r in references],\n",
    "        sources=sources\n",
    "    )\n",
    "    print(f\"[{description}] COMET = {comet_results['mean_score']:.3f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"bleu\": bleu.score,\n",
    "        \"comet\": comet_results[\"mean_score\"]\n",
    "    }\n",
    "\n",
    "###############################\n",
    "# 5) Comment out old load_eval_data\n",
    "###############################\n",
    "# def load_eval_data(num_examples=50):\n",
    "#     print(f\"Loading WMT19 (de-en) validation data with {num_examples} examples...\")\n",
    "#     eval_data = load_dataset(\"wmt19\", \"de-en\", split=\"validation\")\n",
    "#     eval_data = eval_data.select(range(min(num_examples, len(eval_data))))\n",
    "#     return eval_data\n",
    "\n",
    "# Instead, use load_dev_data_local:\n",
    "print(\"Evaluating baseline model using local dev data...\")\n",
    "\n",
    "eval_dataset = load_dev_data_local(num_examples=50)\n",
    "baseline_results_path = \"./results/baseline_results.json\"\n",
    "\n",
    "# Check if baseline results are cached\n",
    "if os.path.exists(baseline_results_path):\n",
    "    print(\"Loading saved baseline results...\")\n",
    "    with open(baseline_results_path, 'r') as f:\n",
    "        baseline_debug_results = json.load(f)\n",
    "    print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "else:\n",
    "    print(\"Evaluating baseline model locally...\")\n",
    "    baseline_debug_results = debug_evaluate_model(\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        eval_dataset=eval_dataset,\n",
    "        num_examples=20,\n",
    "        debug_print=3,\n",
    "        description=\"Baseline LLaMA\"\n",
    "    )\n",
    "    with open(baseline_results_path, 'w') as f:\n",
    "        json.dump(baseline_debug_results, f)\n",
    "\n",
    "print(\"Baseline evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef0e0f-4b22-47ad-955d-42da4211bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_sgm_segments(sgm_file):\n",
    "    \"\"\"\n",
    "    Count how many <seg id=\"...\"> blocks are in the given .sgm file.\n",
    "    \"\"\"\n",
    "    with open(sgm_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    segments = re.findall(r'<seg id=\"\\d+\">(.*?)</seg>', content, flags=re.DOTALL)\n",
    "    return len(segments)\n",
    "de_file = \"wmt_dataset/dev/newstest2018-deen-src.de.sgm\"\n",
    "en_file = \"wmt_dataset/dev/newstest2018-deen-ref.en.sgm\"\n",
    "\n",
    "de_count = count_sgm_segments(de_file)\n",
    "en_count = count_sgm_segments(en_file)\n",
    "\n",
    "print(f\"German file has {de_count} segments.\")\n",
    "print(f\"English file has {en_count} segments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc3bad-16bf-4eba-aa93-2753d996aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_sgm_segments(sgm_file):\n",
    "    \"\"\"\n",
    "    Count how many <seg id=\"...\"> blocks are in the given .sgm file.\n",
    "    \"\"\"\n",
    "    with open(sgm_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    segments = re.findall(r'<seg id=\"\\d+\">(.*?)</seg>', content, flags=re.DOTALL)\n",
    "    return len(segments)\n",
    "de_file = \"wmt_dataset/test/sgm/newstest2019-deen-src.de.sgm\"\n",
    "en_file = \"wmt_dataset/test/sgm/newstest2019-deen-ref.en.sgm\"\n",
    "\n",
    "de_count = count_sgm_segments(de_file)\n",
    "en_count = count_sgm_segments(en_file)\n",
    "\n",
    "print(f\"German file has {de_count} segments.\")\n",
    "print(f\"English file has {en_count} segments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452c598-3393-4026-9938-bd2dd3eadfe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CELL 4\n",
    "def build_full_text(example):\n",
    "    german = example[\"translation\"][\"de\"]\n",
    "    english = example[\"translation\"][\"en\"]\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    full_text = prompt + \" \" + english\n",
    "    return {\"full_text\": full_text}\n",
    "\n",
    "def load_and_format_wmt(num_examples=10000): \n",
    "    print(f\"Loading WMT19 (de-en) train data with {num_examples} examples...\")\n",
    "    dataset = load_dataset(\"wmt19\", \"de-en\", split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42).select(range(num_examples))\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        build_full_text,\n",
    "        desc=\"Building prompt + target text\",\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "class PromptMaskCollator:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        texts = [ex[\"full_text\"] for ex in examples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if \"English translation:\" in text:\n",
    "                prompt_part, _ = text.split(\"English translation:\", 1)\n",
    "                prompt_part = prompt_part + \"English translation:\"\n",
    "            else:\n",
    "                prompt_part = text\n",
    "            \n",
    "            prompt_ids = self.tokenizer(\n",
    "                prompt_part,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            prompt_len = len(prompt_ids)\n",
    "            if prompt_len > labels.size(1):\n",
    "                prompt_len = labels.size(1)\n",
    "            \n",
    "            labels[i, :prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "training_data = load_and_format_wmt(num_examples=10000)\n",
    "data_collator = PromptMaskCollator(tokenizer, max_length=512)\n",
    "\n",
    "print(\"Training data prepared with prompt masking.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afa863-4e0e-43bb-bb3a-a8e16e93c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel \n",
    "\n",
    "def setup_lora_model():\n",
    "    print(\"Setting up LoRA model...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False \n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,                   \n",
    "        lora_alpha=32,         \n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,   \n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \n",
    "            \"v_proj\", \n",
    "            \"k_proj\", \n",
    "            \"o_proj\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    for name, param in lora_model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "    return lora_model\n",
    "\n",
    "model_for_training = setup_lora_model()\n",
    "print(\"LoRA model is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7263710-7008-4c6e-9bac-80419f38d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6\n",
    "import shutil\n",
    "if os.path.exists(\"./my_results\"):\n",
    "    print(\"Cleaning up old results directory...\")\n",
    "    shutil.rmtree(\"./my_results\")\n",
    "os.makedirs(\"./my_results\")\n",
    "\n",
    "eval_data = load_and_format_wmt(num_examples=1000) \n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./my_results\",\n",
    "    num_train_epochs=4,          \n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.15,\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    load_best_model_at_end=True,  \n",
    "    evaluation_strategy=\"steps\",  \n",
    "    eval_steps=500,              \n",
    "    save_total_limit=2,          \n",
    "    metric_for_best_model=\"loss\" \n",
    ")\n",
    "\n",
    "print(f\"Training arguments set. Will train for {train_args.num_train_epochs} epochs on ~{len(training_data)} examples.\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=train_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=eval_data,       \n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "train_output = trainer.train()\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "print(train_output)\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./my_results/lora_7b\")\n",
    "print(\"Fine-tuning done. Model saved at ./my_results/lora_7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b31f58-871c-438e-9c82-67539e2d8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model_save_path = \"./saved_models/base_llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba704a9-af91-4fe7-87e2-e8563dd04290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "def load_lora_model(checkpoint_path=\"./my_results/lora_7b\"):\n",
    "    print(f\"Loading LoRA model from {checkpoint_path}...\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_save_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    lora_model_loaded = PeftModel.from_pretrained(base, checkpoint_path)\n",
    "    lora_model_loaded = lora_model_loaded.merge_and_unload()\n",
    "    return lora_model_loaded\n",
    "\n",
    "print(\"\\nEvaluating LoRA-Fine-Tuned Model...\")\n",
    "merged_model = load_lora_model(\"./my_results/lora_7b\")\n",
    "\n",
    "# We'll reuse the same eval_dataset from our local dev approach\n",
    "# i.e. the one in Cell 3. But if you want to re-load it:\n",
    "# eval_dataset = load_dev_data_local(num_examples=50)\n",
    "\n",
    "lora_debug_results = debug_evaluate_model(\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"LoRA Fine-Tuned\"\n",
    ")\n",
    "\n",
    "print(\"\\nFinal comparison:\")\n",
    "print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "print(f\"LoRA     -> BLEU = {lora_debug_results['bleu']:.2f}, COMET = {lora_debug_results['comet']:.3f}\")\n",
    "\n",
    "# Save LoRA results\n",
    "lora_results_path = \"./results/lora_results.json\"\n",
    "with open(lora_results_path, 'w') as f:\n",
    "    json.dump(lora_debug_results, f)\n",
    "print(f\"LoRA results saved to {lora_results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87d621-fd30-4b2a-ac69-e071c8963312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8\n",
    "import json\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_samples = 50\n",
    "data_for_labeling = load_dataset(\"wmt19\", \"de-en\", split=\"validation\").select(range(num_samples))\n",
    "\n",
    "print(f\"Loaded {len(data_for_labeling)} examples for labeling.\")\n",
    "\n",
    "comet_metric = evaluate.load(\"comet\")\n",
    "\n",
    "def get_quality_label(score):\n",
    "    if score < 0.2:\n",
    "        return \"Bad\"\n",
    "    elif score < 0.6:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Good\"\n",
    "\n",
    "labeled_examples = []\n",
    "for ex in tqdm(data_for_labeling, desc=\"Generating & Labeling\"):\n",
    "    src_de = ex[\"translation\"][\"de\"]\n",
    "    ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "    prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).to(base_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=4,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"English translation:\" in full_output_text:\n",
    "        pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "    else:\n",
    "        pred_en = full_output_text.strip()\n",
    "\n",
    "    comet_scores = comet_metric.compute(\n",
    "        predictions=[pred_en],\n",
    "        references=[ref_en],\n",
    "        sources=[src_de],\n",
    "        gpus=0,\n",
    "        progress_bar=False\n",
    "    )\n",
    "    score = comet_scores[\"scores\"][0]\n",
    "\n",
    "    label = get_quality_label(score)\n",
    "\n",
    "    labeled_examples.append({\n",
    "        \"source_de\": src_de,\n",
    "        \"reference_en\": ref_en,\n",
    "        \"baseline_translation\": pred_en,\n",
    "        \"comet_score\": float(score),\n",
    "        \"quality_label\": label\n",
    "    })\n",
    "\n",
    "output_file = \"./results/baseline_labeled_translations.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labeled_examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nDone. Saved {len(labeled_examples)} labeled examples to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c67e3-b718-42fd-83cb-b89dcfcc8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def export_to_csv(\n",
    "    input_file=\"./results/baseline_labeled_translations.json\",\n",
    "    output_file=\"./results/errorful_data.csv\"\n",
    "):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_data = json.load(f)\n",
    "\n",
    "    errorful_data = [ex for ex in all_data if ex[\"quality_label\"] in [\"Bad\", \"Medium\"]]\n",
    "    print(f\"Total data: {len(all_data)}\")\n",
    "    print(f\"Bad/Medium examples: {len(errorful_data)}\")\n",
    "\n",
    "    fieldnames = [\n",
    "        \"source_de\",\n",
    "        \"reference_en\",\n",
    "        \"baseline_translation\",\n",
    "        \"comet_score\",\n",
    "        \"quality_label\",\n",
    "        \"error_category\",\n",
    "        \"fix_explanation\",\n",
    "        \"corrected_en\"\n",
    "    ]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for ex in errorful_data:\n",
    "            writer.writerow({\n",
    "                \"source_de\": ex[\"source_de\"],\n",
    "                \"reference_en\": ex[\"reference_en\"],\n",
    "                \"baseline_translation\": ex[\"baseline_translation\"],\n",
    "                \"comet_score\": ex[\"comet_score\"],\n",
    "                \"quality_label\": ex[\"quality_label\"],\n",
    "                \"error_category\": \"\",\n",
    "                \"fix_explanation\": \"\",\n",
    "                \"corrected_en\": \"\"\n",
    "            })\n",
    "\n",
    "    print(f\"\\nCSV created at: {output_file}\")\n",
    "\n",
    "export_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722e995-2ef8-44e5-8b87-7f702ff10144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
