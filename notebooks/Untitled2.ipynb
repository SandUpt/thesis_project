{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bbbd12b-0230-43bf-9ba4-293660b3229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.2\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 1: Initial imports and environment setup\n",
    "# --------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdd547a-bcad-4b49-ac29-ec907f06ff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading base model in float16 with device_map='auto'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22488ba0ce4c4362a219ad8ac3b8d234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 2: Load Base Model & Tokenizer (Baseline)\n",
    "# --------------------------------------------\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading base model in float16 with device_map='auto'...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Ensure a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Baseline model and tokenizer loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9fb1b0-4642-44fc-b78c-b6c6f2031ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT19 (de-en) train data with 100000 examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ba4594884f4a1da19bc84ed8de2d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building prompt + target text:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data prepared with prompt masking.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 3: Prepare WMT19 Training Data (Prompt Masking)\n",
    "# --------------------------------------------\n",
    "def build_full_text(example):\n",
    "    german = example[\"translation\"][\"de\"]\n",
    "    english = example[\"translation\"][\"en\"]\n",
    "    prompt = (\n",
    "        \"Translate this German text into fluent English.\\n\"\n",
    "        f\"{german}\\n\"\n",
    "        \"Translation:\"\n",
    "    )\n",
    "    full_text = prompt + \" \" + english\n",
    "    return {\"full_text\": full_text}\n",
    "\n",
    "def load_and_format_wmt(num_examples=10000):\n",
    "    print(f\"Loading WMT19 (de-en) train data with {num_examples} examples...\")\n",
    "    dataset = load_dataset(\"wmt19\", \"de-en\", split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42).select(range(num_examples))\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        build_full_text,\n",
    "        desc=\"Building prompt + target text\",\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "class PromptMaskCollator:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        texts = [ex[\"full_text\"] for ex in examples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if \"Translation:\" in text:\n",
    "                prompt_part, _ = text.split(\"Translation:\", 1)\n",
    "                prompt_part = prompt_part + \"Translation:\"\n",
    "            else:\n",
    "                prompt_part = text\n",
    "            \n",
    "            prompt_ids = self.tokenizer(\n",
    "                prompt_part,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            prompt_len = len(prompt_ids)\n",
    "            if prompt_len > labels.size(1):\n",
    "                prompt_len = labels.size(1)\n",
    "            \n",
    "            # Mask out prompt tokens\n",
    "            labels[i, :prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Load training data for fine-tuning\n",
    "training_data = load_and_format_wmt(num_examples=10000)\n",
    "data_collator = PromptMaskCollator(tokenizer, max_length=512)\n",
    "\n",
    "print(\"Training data prepared with prompt masking.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c77c2e3-0822-422b-8298-c063c0ce3c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LoRA model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebda53b40c440c7949c7c653a3bedbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n",
      "LoRA model is ready.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 4: LoRA Configuration and Model\n",
    "# --------------------------------------------\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def setup_lora_model():\n",
    "    print(\"Setting up LoRA model...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    lora_base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Freeze embeddings\n",
    "    for param in lora_base.get_input_embeddings().parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(lora_base, lora_config)\n",
    "    lora_model.print_trainable_parameters()\n",
    "    return lora_model\n",
    "\n",
    "model_for_training = setup_lora_model()\n",
    "\n",
    "print(\"LoRA model is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f506b55-812d-47c7-8ef6-f1b36f0c66d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments set. Output will be saved to ./my_results\n",
      "Planned: 3 epochs on ~100000 examples.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 5: Training Arguments\n",
    "# --------------------------------------------\n",
    "os.makedirs(\"./my_results\", exist_ok=True)  \n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./my_results\",    \n",
    "    num_train_epochs=2,           # 1 epoch over 50k lines\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=250,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "print(\"Training arguments set. Output will be saved to ./my_results\")\n",
    "print(f\"Planned: {train_args.num_train_epochs} epochs on ~{len(training_data)} examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49865886-09ba-4f2c-b71e-268fd417209a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA fine-tuning...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 10:31:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>30.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.056900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished.\n",
      "Training metrics:\n",
      "TrainOutput(global_step=9375, training_loss=0.7584611862182618, metrics={'train_runtime': 37908.6319, 'train_samples_per_second': 7.914, 'train_steps_per_second': 0.247, 'total_flos': 6.0970588176384e+18, 'train_loss': 0.7584611862182618, 'epoch': 3.0})\n",
      "Fine-tuning done. Model saved at ./my_results/lora_7b\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 6: Initialize Trainer & Start Fine-tuning\n",
    "# --------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=train_args,\n",
    "    train_dataset=training_data,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\\n\")\n",
    "train_output = trainer.train()\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "print(train_output)\n",
    "\n",
    "# Save the final fine-tuned LoRA model\n",
    "trainer.save_model(\"./my_results/lora_7b\")\n",
    "print(\"Fine-tuning done. Model saved at ./my_results/lora_7b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c73a7d-996a-43eb-8919-5c7c7be6a426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT19 (de-en) validation data with 500 examples...\n",
      "Validation data loaded.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 7: Load Evaluation Data\n",
    "# --------------------------------------------\n",
    "def load_eval_data(num_examples=500):\n",
    "    print(f\"Loading WMT19 (de-en) validation data with {num_examples} examples...\")\n",
    "    eval_data = load_dataset(\"wmt19\", \"de-en\", split=\"validation\")\n",
    "    eval_data = eval_data.select(range(min(num_examples, len(eval_data))))\n",
    "    return eval_data\n",
    "\n",
    "eval_dataset = load_eval_data(num_examples=50)  \n",
    "\n",
    "print(\"Validation data loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd216124-82f0-44ea-ac8e-2b48cc50d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 8: Debugging-Aware Evaluation Function\n",
    "# --------------------------------------------\n",
    "def build_prompt_for_translation(german_text: str) -> str:\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a professional translator specializing in German to English.\\n\\n\"\n",
    "        \"Below is some text in German. Translate it into fluent, idiomatic English.\\n\\n\"\n",
    "        f\"German text:\\n{german_text}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def debug_evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    num_examples=50,      \n",
    "    debug_print=5,        \n",
    "    description=\"Model\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the given `model` on `eval_dataset` with a debugging-friendly process:\n",
    "      - Prints the exact prompt\n",
    "      - Prints the tokenized input (input_ids)\n",
    "      - Prints the full model output (with special tokens)\n",
    "      - Prints the final extracted translation\n",
    "    Then computes BLEU & COMET metrics as a rough measure of translation quality.\n",
    "    \"\"\"\n",
    "    comet_metric = evaluate.load(\"comet\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    sources = []\n",
    "\n",
    "    subset = eval_dataset.select(range(min(num_examples, len(eval_dataset))))\n",
    "    print(f\"\\n[DEBUG EVAL] {description} on {num_examples} examples...\\n\")\n",
    "\n",
    "    for i, ex in enumerate(tqdm(subset, desc=f\"Evaluating {description}\")):\n",
    "        src_de = ex[\"translation\"][\"de\"]\n",
    "        ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "        prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "        tokenized_input = tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **tokenized_input,\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "\n",
    "        if \"English translation:\" in full_output_text:\n",
    "            pred_en = full_output_text.rsplit(\"English translation:\", 1)[-1].strip()\n",
    "        else:\n",
    "            pred_en = full_output_text\n",
    "\n",
    "        predictions.append(pred_en)\n",
    "        references.append([ref_en])\n",
    "        sources.append(src_de)\n",
    "\n",
    "        if i < debug_print:\n",
    "            print(\"\\n==========================================\")\n",
    "            print(f\"Example {i}\")\n",
    "            print(\"---------------[ PROMPT ]-----------------\")\n",
    "            print(prompt_text)\n",
    "            print(\"--------------[ TOKENIZED ]---------------\")\n",
    "            print(f\"Input IDs: {tokenized_input['input_ids'][0].tolist()}\")\n",
    "            print(\"-----------[ FULL MODEL OUTPUT ]----------\")\n",
    "            print(repr(full_output_text))\n",
    "            print(\"-------------[ EXTRACTED EN ]-------------\")\n",
    "            print(repr(pred_en))\n",
    "            print(\"--------------[ REFERENCE ]---------------\")\n",
    "            print(ref_en)\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "    # 6) Compute BLEU\n",
    "    bleu = corpus_bleu(predictions, references)\n",
    "    print(f\"[{description}] BLEU = {bleu.score:.2f}\")\n",
    "\n",
    "    comet_results = comet_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=[r[0] for r in references],\n",
    "        sources=sources\n",
    "    )\n",
    "    print(f\"[{description}] COMET = {comet_results['mean_score']:.3f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"bleu\": bleu.score,\n",
    "        \"comet\": comet_results[\"mean_score\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8547f90-5705-41a7-973f-b7e319654d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline Model with Debug Info...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf9edafe5c145ecad81be6ba72e1c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG EVAL] Baseline LLaMA on 50 examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA:   0%|                                                                 | 0/50 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating Baseline LLaMA:   2%|█▏                                                       | 1/50 [00:04<03:24,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 0\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are a professional translator specializing in German to English.\n",
      "\n",
      "Below is some text in German. Translate it into fluent, idiomatic English.\n",
      "\n",
      "German text:\n",
      "München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 263, 10257, 5578, 1061, 4266, 5281, 297, 5332, 304, 4223, 29889, 13, 13, 21140, 340, 338, 777, 1426, 297, 5332, 29889, 4103, 9632, 372, 964, 1652, 8122, 29892, 1178, 14910, 2454, 4223, 29889, 13, 13, 29954, 3504, 1426, 29901, 13, 29924, 3346, 2724, 29871, 29896, 29947, 29945, 29953, 29901, 23650, 476, 8109, 29892, 762, 306, 13608, 350, 1406, 1622, 762, 5587, 1147, 3140, 824, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'<s> You are a professional translator specializing in German to English.\\n\\nBelow is some text in German. Translate it into fluent, idiomatic English.\\n\\nGerman text:\\nMünchen 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\\n\\nEnglish translation:\\nMunich 1856: Four cards that change your view of the city\\n</s>'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Munich 1856: Four cards that change your view of the city\\n</s>'\n",
      "--------------[ REFERENCE ]---------------\n",
      "Munich 1856: Four maps that will change your view of the city\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA:   4%|██▎                                                      | 2/50 [00:04<01:38,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 1\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are a professional translator specializing in German to English.\n",
      "\n",
      "Below is some text in German. Translate it into fluent, idiomatic English.\n",
      "\n",
      "German text:\n",
      "Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 263, 10257, 5578, 1061, 4266, 5281, 297, 5332, 304, 4223, 29889, 13, 13, 21140, 340, 338, 777, 1426, 297, 5332, 29889, 4103, 9632, 372, 964, 1652, 8122, 29892, 1178, 14910, 2454, 4223, 29889, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 6600, 1267, 29899, 2744, 303, 1997, 29892, 8879, 2160, 12843, 19472, 4545, 1812, 387, 4566, 899, 2435, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "\"<s> You are a professional translator specializing in German to English.\\n\\nBelow is some text in German. Translate it into fluent, idiomatic English.\\n\\nGerman text:\\nEine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\\n\\nEnglish translation:\\nAn insane asylum, where today's youth should meet.\\n</s>\"\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "\"An insane asylum, where today's youth should meet.\\n</s>\"\n",
      "--------------[ REFERENCE ]---------------\n",
      "A mental asylum, where today young people are said to meet.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA:   6%|███▍                                                     | 3/50 [00:05<01:06,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 2\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are a professional translator specializing in German to English.\n",
      "\n",
      "Below is some text in German. Translate it into fluent, idiomatic English.\n",
      "\n",
      "German text:\n",
      "Eine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 263, 10257, 5578, 1061, 4266, 5281, 297, 5332, 304, 4223, 29889, 13, 13, 21140, 340, 338, 777, 1426, 297, 5332, 29889, 4103, 9632, 372, 964, 1652, 8122, 29892, 1178, 14910, 2454, 4223, 29889, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 5430, 615, 21474, 1808, 29892, 8879, 11923, 1865, 972, 317, 29899, 29933, 5422, 29899, 29911, 16163, 21598, 336, 1785, 4296, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'<s> You are a professional translator specializing in German to English.\\n\\nBelow is some text in German. Translate it into fluent, idiomatic English.\\n\\nGerman text:\\nEine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\\n\\nEnglish translation:\\nA graveyard chapel, where now a S-Bahn tunnel is being dug.\\n</s>'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'A graveyard chapel, where now a S-Bahn tunnel is being dug.\\n</s>'\n",
      "--------------[ REFERENCE ]---------------\n",
      "A crypt chapel, where they are now digging tunnels for the S-Bahn.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA: 100%|████████████████████████████████████████████████████████| 50/50 [01:12<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline LLaMA] BLEU = 47.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline LLaMA] COMET = 0.808\n",
      "\n",
      "Evaluating LoRA-Fine-Tuned Model with Debug Info...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914575d1dff049099e6092105b91ce26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG EVAL] LoRA Fine-Tuned on 50 examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned:   0%|                                                                | 0/50 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating LoRA Fine-Tuned:   2%|█                                                       | 1/50 [00:01<00:59,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 0\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are a professional translator specializing in German to English.\n",
      "\n",
      "Below is some text in German. Translate it into fluent, idiomatic English.\n",
      "\n",
      "German text:\n",
      "München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 263, 10257, 5578, 1061, 4266, 5281, 297, 5332, 304, 4223, 29889, 13, 13, 21140, 340, 338, 777, 1426, 297, 5332, 29889, 4103, 9632, 372, 964, 1652, 8122, 29892, 1178, 14910, 2454, 4223, 29889, 13, 13, 29954, 3504, 1426, 29901, 13, 29924, 3346, 2724, 29871, 29896, 29947, 29945, 29953, 29901, 23650, 476, 8109, 29892, 762, 306, 13608, 350, 1406, 1622, 762, 5587, 1147, 3140, 824, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'<s> You are a professional translator specializing in German to English.\\n\\nBelow is some text in German. Translate it into fluent, idiomatic English.\\n\\nGerman text:\\nMünchen 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\\n\\nEnglish translation:\\nMunich 1856: Four cards that change your view of the city</s>'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Munich 1856: Four cards that change your view of the city</s>'\n",
      "--------------[ REFERENCE ]---------------\n",
      "Munich 1856: Four maps that will change your view of the city\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned:   4%|██▏                                                     | 2/50 [00:02<00:48,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 1\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are a professional translator specializing in German to English.\n",
      "\n",
      "Below is some text in German. Translate it into fluent, idiomatic English.\n",
      "\n",
      "German text:\n",
      "Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 263, 10257, 5578, 1061, 4266, 5281, 297, 5332, 304, 4223, 29889, 13, 13, 21140, 340, 338, 777, 1426, 297, 5332, 29889, 4103, 9632, 372, 964, 1652, 8122, 29892, 1178, 14910, 2454, 4223, 29889, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 6600, 1267, 29899, 2744, 303, 1997, 29892, 8879, 2160, 12843, 19472, 4545, 1812, 387, 4566, 899, 2435, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'<s> You are a professional translator specializing in German to English.\\n\\nBelow is some text in German. Translate it into fluent, idiomatic English.\\n\\nGerman text:\\nEine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\\n\\nEnglish translation:\\nAn asylum for the insane, where young people should meet today.</s>'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'An asylum for the insane, where young people should meet today.</s>'\n",
      "--------------[ REFERENCE ]---------------\n",
      "A mental asylum, where today young people are said to meet.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned:   6%|███▎                                                    | 3/50 [00:03<00:46,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 2\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are a professional translator specializing in German to English.\n",
      "\n",
      "Below is some text in German. Translate it into fluent, idiomatic English.\n",
      "\n",
      "German text:\n",
      "Eine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 263, 10257, 5578, 1061, 4266, 5281, 297, 5332, 304, 4223, 29889, 13, 13, 21140, 340, 338, 777, 1426, 297, 5332, 29889, 4103, 9632, 372, 964, 1652, 8122, 29892, 1178, 14910, 2454, 4223, 29889, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 5430, 615, 21474, 1808, 29892, 8879, 11923, 1865, 972, 317, 29899, 29933, 5422, 29899, 29911, 16163, 21598, 336, 1785, 4296, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'<s> You are a professional translator specializing in German to English.\\n\\nBelow is some text in German. Translate it into fluent, idiomatic English.\\n\\nGerman text:\\nEine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\\n\\nEnglish translation:\\nA crypt chapel, where now the S-Bahn tunnel is being dug.</s>'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'A crypt chapel, where now the S-Bahn tunnel is being dug.</s>'\n",
      "--------------[ REFERENCE ]---------------\n",
      "A crypt chapel, where they are now digging tunnels for the S-Bahn.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned: 100%|███████████████████████████████████████████████████████| 50/50 [01:15<00:00,  1.50s/it]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA Fine-Tuned] BLEU = 47.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA Fine-Tuned] COMET = 0.808\n",
      "\n",
      "Comparison of final metrics:\n",
      "Baseline -> BLEU = 47.92, COMET = 0.808\n",
      "LoRA     -> BLEU = 47.92, COMET = 0.808\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 9: Compare Baseline vs. LoRA-Fine-Tuned\n",
    "# --------------------------------------------\n",
    "print(\"Evaluating Baseline Model with Debug Info...\")\n",
    "baseline_debug_results = debug_evaluate_model(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_examples=20,    \n",
    "    debug_print=3,      \n",
    "    description=\"Baseline LLaMA\"\n",
    ")\n",
    "\n",
    "print(\"Evaluating LoRA-Fine-Tuned Model with Debug Info...\")\n",
    "lora_debug_results = debug_evaluate_model(\n",
    "    model=model_for_training,  \n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"LoRA Fine-Tuned\"\n",
    ")\n",
    "\n",
    "print(\"Comparison of final metrics:\")\n",
    "print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "print(f\"LoRA     -> BLEU = {lora_debug_results['bleu']:.2f}, COMET = {lora_debug_results['comet']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2dba2dc-1ab3-4ce6-8296-9b3fbc8b5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 10: Resume from Saved LoRA Checkpoint\n",
    "# --------------------------------------------\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_lora_model(checkpoint_path=\"./my_results/lora_7b\"):\n",
    "    \"\"\"\n",
    "    Loads the saved LoRA weights from disk, merges them with the base model, \n",
    "    and returns a ready-for-inference model.\n",
    "    \"\"\"\n",
    "    print(f\"Loading LoRA model from {checkpoint_path}...\")\n",
    "    # Load base again\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Merge LoRA\n",
    "    lora_model_loaded = PeftModel.from_pretrained(base, checkpoint_path)\n",
    "    return lora_model_loaded\n",
    "\n",
    "# resumed_lora_model = load_lora_model(\"./my_results/lora_7b\")\n",
    "# debug_evaluate_model(resumed_lora_model, tokenizer, eval_dataset, num_examples=10, description=\"Resumed LoRA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de072d3-addc-41d6-9353-8e0edb44e0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID of base_model: 139838127032592\n",
      "ID of model_for_training: 139838134663760\n"
     ]
    }
   ],
   "source": [
    "print(\"ID of base_model:\", id(base_model))\n",
    "print(\"ID of model_for_training:\", id(model_for_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71224a77-3188-4dfc-b628-35d8264c2b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
